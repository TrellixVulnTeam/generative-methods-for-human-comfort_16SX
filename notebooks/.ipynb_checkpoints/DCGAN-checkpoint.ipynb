{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T07:05:57.949299Z",
     "start_time": "2019-07-20T07:05:57.909529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "#!pip install tensorflow-gpu==2.0.0-beta1\n",
    "\n",
    "\"\"\"\n",
    "Source: https://github.com/matiRLC/Keras-GAN/blob/master/gan/gan.py\n",
    "Adapted by: matias@u.nus.edu\n",
    "Updates:\n",
    "    Try CGAN\n",
    "    supervised discriminator\n",
    "    Wasserstein GAN\n",
    "\"\"\"\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from IPython import display \n",
    "print(\"Tensorflow version {}\".format(tf.__version__))\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Numpy, pandas, matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import PIL\n",
    "\n",
    "# Sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "class DCGAN():\n",
    "    def __init__(self, n_features, data_dim=28, n_hidden=150, n_layers=2, lr= 0.0001, display=False):\n",
    "        self.display = display\n",
    "        self.seed = 13 # change if necessary\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.n_features = n_features # number of features in the dataset\n",
    "        self.data_dim = data_dim # dimension of the square image\n",
    "        self.latent_dim = 100\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # optimizers\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        \n",
    "        # helper function to computer cross entropy loss\n",
    "        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        \n",
    "        # loss arrays\n",
    "        self.loss_g = []\n",
    "        self.loss_d = []\n",
    "        \n",
    "    def build_generator(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(self.latent_dim,)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU())\n",
    "\n",
    "        model.add(layers.Reshape((7, 7, 256)))\n",
    "        assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
    "\n",
    "        model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "        assert model.output_shape == (None, 7, 7, 128)\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU())\n",
    "\n",
    "        model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "        assert model.output_shape == (None, 14, 14, 64)\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU())\n",
    "\n",
    "        model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "        assert model.output_shape == (None, self.data_dim, self.data_dim, 1)\n",
    "\n",
    "        print(\"Generator Summary:\")\n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                         input_shape=[self.data_dim, self.data_dim, 1]))\n",
    "        model.add(layers.LeakyReLU())\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "        model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "        model.add(layers.LeakyReLU())\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1))\n",
    "\n",
    "        print(\"Discriminator Summary:\")\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        real_loss = self.cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        fake_loss = self.cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        total_loss = real_loss + fake_loss\n",
    "        return total_loss\n",
    "    \n",
    "    # TODO: add correlation loss, of mean and std\n",
    "    def generator_loss(self, fake_output):\n",
    "        return self.cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    \n",
    "    # This annotation causes the function to be \"compiled\" and therefore run as graph\n",
    "    @tf.function \n",
    "    def train_step(self, data, BATCH_SIZE):\n",
    "        noise = tf.random.uniform([BATCH_SIZE, self.latent_dim], -1, 1)\n",
    "#         noise = tf.random.normal([BATCH_SIZE, self.latent_dim])\n",
    "\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            # TODO: pre-train the generator before adversarial training\n",
    "            generated_data = self.generator(noise, training=True)\n",
    "\n",
    "            real_output = self.discriminator(data, training=True)\n",
    "            fake_output = self.discriminator(generated_data, training=True)\n",
    "\n",
    "            gen_loss = self.generator_loss(fake_output)\n",
    "            disc_loss = self.discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, \n",
    "                                                   self.generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, \n",
    "                                                        self.discriminator.trainable_variables)\n",
    "\n",
    "        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, \n",
    "                                                self.generator.trainable_variables))\n",
    "        self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, \n",
    "                                                    self.discriminator.trainable_variables))\n",
    "        return gen_loss, disc_loss\n",
    "        \n",
    "    def train(self, dataset, EPOCHS, BATCH_SIZE=128, SAMPLE_INTERVAL=15):\n",
    "        origin_dataset = dataset.copy()\n",
    "        # transform dataset into images of data\n",
    "        dataset = self.prepare_dataset(dataset, BATCH_SIZE) # TODO: UPDATE THIS AS DATASET\n",
    "        \n",
    "        # progress bar\n",
    "        pbar = tqdm(total=EPOCHS)\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            for data_batch in dataset:\n",
    "                gen_loss, disc_loss = self.train_step(data_batch, BATCH_SIZE)\n",
    "            \n",
    "            # Save the model every SAMPLE_INTERVAL epochs\n",
    "            if epoch % SAMPLE_INTERVAL == 0:\n",
    "                self.generate_data(origin_dataset, epoch, BATCH_SIZE)\n",
    "                display.clear_output(wait=True)\n",
    "             #   checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            \n",
    "            # update losses\n",
    "            self.loss_g.append(gen_loss.numpy())\n",
    "            self.loss_d.append(disc_loss.numpy())\n",
    "#             print(\"D Loss: {0:.2f}, G Loss: {0:.2f}\".format(self.loss_d[-1], self.loss_g[-1]))   \n",
    "\n",
    "            # progress bar\n",
    "            pbar.update(1)\n",
    "            \n",
    "        # Generate after the final epoch\n",
    "        display.clear_output(wait=True)\n",
    "        self.generate_data(origin_dataset, epoch, BATCH_SIZE)\n",
    "    \n",
    "        pbar.close()\n",
    "    \n",
    "    def generate_data(self, origin_dataset, epoch=1, BATCH_SIZE=128):\n",
    "        # TODO: change for uniform -1,1\n",
    "        noise = tf.random.uniform([BATCH_SIZE * 5, self.latent_dim], -1, 1)\n",
    "#         noise = tf.random.normal([BATCH_SIZE * 5, self.latent_dim])        \n",
    "        generated_x = self.generator(noise, training=False).numpy()\n",
    "\n",
    "        if self.display:\n",
    "            fig = plt.figure()\n",
    "            plt.hist(generated_x.numpy(), bins=40, density=True, histtype='bar')\n",
    "            plt.title(\"testing:\" + str(epoch))\n",
    "            plt.show()\n",
    "            fig.savefig(\"../output/GANtest/{}.png\".format(epoch))\n",
    "        \n",
    "        generated_x = generated_x.reshape(generated_x.shape[0], generated_x.shape[1] * generated_x.shape[2])\n",
    "\n",
    "        # Selecting the correct number of atributes (used in training)\n",
    "        generated_x = generated_x[:, : self.n_features]\n",
    "\n",
    "        print(\"Synth Data shape= \" + str(generated_x.shape))\n",
    "\n",
    "        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "        min_max_scaler.fit(origin_dataset)\n",
    "        generated_x = min_max_scaler.inverse_transform(generated_x)\n",
    "        \n",
    "        generated_x = pd.DataFrame(generated_x, columns=origin_dataset.columns.values)\n",
    "        generated_x['Gender'] = round(generated_x['Gender'])\n",
    "        generated_x['Discrete Thermal Comfort_TA'] = round(generated_x['Discrete Thermal Comfort_TA'])\n",
    "        \n",
    "        return generated_x\n",
    "    \n",
    "    def get_losses(self):\n",
    "        \"\"\"\n",
    "        Return loses\n",
    "        \"\"\"\n",
    "        return self.loss_g , self.loss_d\n",
    "    \n",
    "    def padding_duplicating(self, data, row_size):\n",
    "        arr_data = np.array(data.values.tolist())\n",
    "\n",
    "        col_num = arr_data.shape[1]\n",
    "\n",
    "        npad = ((0, 0), (0, row_size - col_num))\n",
    "\n",
    "        # padding with zero\n",
    "        arr_data = np.pad(arr_data, pad_width=npad, mode='constant', constant_values=0.)\n",
    "\n",
    "        # duplicating Values \n",
    "        for i in range(1, arr_data.shape[1] // col_num):\n",
    "            arr_data[:, col_num * i: col_num * (i + 1)] = arr_data[:, 0: col_num]\n",
    "\n",
    "        return arr_data\n",
    "    \n",
    "    def reshape(self, data, dim):\n",
    "        data = data.reshape(data.shape[0], dim, -1)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def prepare_dataset(self, dataframe, BATCH_SIZE=128):\n",
    "        \"\"\"\n",
    "        Transform dataframe to images\n",
    "        \"\"\"\n",
    "        BUFFER_SIZE = dataframe.shape[0] * 2\n",
    "        # rescale everything between [-1,1]\n",
    "        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "        # normalizing dat\n",
    "        X = pd.DataFrame(min_max_scaler.fit_transform(dataframe))\n",
    "        # padding to match square image size\n",
    "        padded_ar = self.padding_duplicating(X, self.data_dim * self.data_dim)\n",
    "        # reshape dataset\n",
    "        X = self.reshape(padded_ar, self.data_dim)\n",
    "        print(\"Final Real Data shape = \" + str(X.shape))\n",
    "        # final shape for image        \n",
    "        X = X.reshape(X.shape[0], self.data_dim, self.data_dim, 1).astype('float32')\n",
    "        print(\"Final Real Data shape = \" + str(X.shape))\n",
    "\n",
    "        # shuffle and create batches\n",
    "        X = tf.data.Dataset.from_tensor_slices(X).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
