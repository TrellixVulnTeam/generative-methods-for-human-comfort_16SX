{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T01:29:18.486546Z",
     "start_time": "2019-07-17T01:29:17.789257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas, matplotlib, random\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Notebooks\n",
    "import nbimporter\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from GAN import GAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T14:52:26.115192Z",
     "start_time": "2019-07-30T14:52:26.085972Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_GAN(dataframe, parameters):\n",
    "    # unpack parameters\n",
    "    use_corr_loss = parameters['use_corr_loss']\n",
    "    BATCH_SIZE = parameters['BATCH_SIZE']\n",
    "    data_dim = 15\n",
    "    n_hidden = parameters['n_hidden']\n",
    "    n_layers = parameters['n_layers']\n",
    "    EPOCHS = parameters['EPOCHS']\n",
    "    lr = parameters['lr']\n",
    "    \n",
    "    print(\"Number of Features: {}\".format(data_dim))\n",
    "    \n",
    "    # cross-validation lists\n",
    "    loss_g_cv = []\n",
    "    loss_d_cv = []\n",
    "    acc_pos_cv = []\n",
    "    acc_neg_cv = []\n",
    "    corr_cv = []\n",
    "    # cross-validation loop\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=13, shuffle=True)\n",
    "    dataframe_array = np.array(dataframe)\n",
    "    for train_index, test_index in cv.split(dataframe_array[:,:-1], dataframe_array[:, -1]):\n",
    "        # get train test split by index slices\n",
    "        df_train, df_test = dataframe.loc[train_index, :], dataframe.loc[test_index, :]\n",
    "\n",
    "        # initiate GAN\n",
    "        gan = GAN(data_dim=data_dim, n_hidden=n_hidden, n_layers=n_layers, lr=lr, display=False)\n",
    "\n",
    "        # Train and sample\n",
    "        gan.train(dataframe=df_train,\n",
    "                  EPOCHS=EPOCHS, \n",
    "                  use_corr_loss=use_corr_loss, \n",
    "                  BATCH_SIZE=BATCH_SIZE, \n",
    "                  SAMPLE_INTERVAL=100)\n",
    "        gen_data = gan.generate_data(epoch=1, BATCH_SIZE=dataframe.shape[0])\n",
    "        \n",
    "        corr = corr_matrix(gen_data) - corr_matrix(df_test)\n",
    "        loss_g, loss_d = gan.get_losses()\n",
    "        acc_pos, acc_neg = gan.get_accuracies()\n",
    "\n",
    "        loss_g_cv.append(loss_g)\n",
    "        loss_d_cv.append(loss_d)\n",
    "        acc_pos_cv.append(acc_pos)\n",
    "        acc_neg_cv.append(acc_neg)\n",
    "        corr_cv.append(np.sum(np.array(corr)))\n",
    "        \n",
    "    return gen_data, np.mean(loss_g_cv), np.mean(loss_d_cv), np.mean(acc_pos_cv), np.mean(acc_neg_cv), np.mean(corr_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T10:28:46.475647Z",
     "start_time": "2019-07-28T10:28:46.469407Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_build_GAN(dataframe, batch_parameters):\n",
    "    # unpacking parameters\n",
    "    use_corr_loss_list = batch_parameters['use_corr_loss']\n",
    "    batch_size_list = batch_parameters['BATCH_SIZE']\n",
    "    n_hidden_list = batch_parameters['n_hidden']\n",
    "    n_layers_list = batch_parameters['n_layers']\n",
    "    epochs_list = batch_parameters['EPOCHS']\n",
    "    lr_list = batch_parameters['lr']\n",
    "    \n",
    "    # list of results\n",
    "    parameters_list = []\n",
    "    gen_df_list = []\n",
    "    loss_g_list = []\n",
    "    loss_d_list = []\n",
    "    acc_pos_list = []\n",
    "    acc_neg_list = []\n",
    "    corr_diff_list = []    \n",
    "    corr_reduced_list = []\n",
    "    \n",
    "    # looping all parameters\n",
    "    for batch_size in batch_size_list:\n",
    "        for n_hidden in n_hidden_list:\n",
    "            for n_layers in n_layers_list:\n",
    "                for use_corr_loss in use_corr_loss_list:\n",
    "                    for epochs in epochs_list:\n",
    "                        for lr in lr_list:\n",
    "                            parameters = {\n",
    "                                'use_corr_loss': use_corr_loss,\n",
    "                                'BATCH_SIZE' : batch_size,\n",
    "                                'n_hidden' : n_hidden,\n",
    "                                'n_layers': n_layers,\n",
    "                                'EPOCHS': epochs,\n",
    "                                'lr': lr\n",
    "                            }\n",
    "\n",
    "                            gen_df, loss_g, loss_d, acc_pos, acc_neg, corr_reduced = build_GAN(dataframe, parameters)\n",
    "                            gen_df_list.append(gen_df)\n",
    "\n",
    "                            loss_g_list.append(loss_g)\n",
    "                            loss_d_list.append(loss_d)\n",
    "\n",
    "                            acc_pos_list.append(acc_pos)\n",
    "                            acc_neg_list.append(acc_neg)\n",
    "\n",
    "                            parameters_list.append(parameters)\n",
    "                            \n",
    "#                             corr = corr_matrix(dataframe) - corr_matrix(gen_df)\n",
    "#                             corr_diff_list.append(corr)\n",
    "#                             corr_reduced_list.append(np.sum(np.array(corr)))\n",
    "\n",
    "                            corr_reduced_list.append(corr_reduced)\n",
    "                            \n",
    "    return gen_df_list, loss_g_list, loss_d_list, acc_pos_list, acc_neg_list, parameters_list, corr_diff_list, corr_reduced_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_matrix(dataframe, get_corr=True, corr=None):\n",
    "    # Compute the correlation matrix\n",
    "    if get_corr:\n",
    "        corr = dataframe.corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    return corr\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
