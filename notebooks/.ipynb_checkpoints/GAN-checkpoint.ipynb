{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T04:30:09.114137Z",
     "start_time": "2019-07-27T04:30:09.066880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "#!pip install tensorflow-gpu==2.0.0-beta1\n",
    "\n",
    "\"\"\"\n",
    "Source: https://github.com/matiRLC/Keras-GAN/blob/master/gan/gan.py\n",
    "Adapted by: matias@u.nus.edu\n",
    "TODOs:\n",
    "    supervised discriminator\n",
    "    Wasserstein GAN\n",
    "    categorical encoding\n",
    "\"\"\"\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from IPython import display \n",
    "print(tf.__version__)\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Numpy, pandas, matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import PIL\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class GAN():\n",
    "    \"\"\"Main model for GAN\n",
    "    Args:\n",
    "        None\n",
    "    \n",
    "    Attributes:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dim=15, n_hidden=200, n_layers=2, lr=0.0002, display=False):\n",
    "        \"\"\"Initiliaze the object, set arguments as attributes.\"\"\"\n",
    "        self.display = display\n",
    "        self.seed = 13\n",
    "        self.gamma = 0.2\n",
    "        self.scaler = None\n",
    "        self.columns = []\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.data_dim = data_dim # number of features\n",
    "        self.latent_dim = 30 # before was data_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # optimizers\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        \n",
    "        # Helper function to computer cross entropy loss\n",
    "        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        \n",
    "        # Initialize correlations with 0\n",
    "        self.prev_gcorr_real = np.zeros((1, self.data_dim), dtype=np.float32)\n",
    "        self.prev_gcorr_fake = np.zeros((1, self.data_dim), dtype=np.float32)\n",
    "        \n",
    "        # Moving average contribution\n",
    "        self.mac = 0.99\n",
    "        \n",
    "        # loss and accuracy arrays\n",
    "        self.loss_g = []\n",
    "        self.loss_d = []\n",
    "        self.acc_pos = []\n",
    "        self.acc_neg = []        \n",
    "        \n",
    "    def build_generator(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        n_hidden= self.n_hidden\n",
    "        n_layers = self.n_layers\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        # first layer\n",
    "        model.add(layers.Dense(n_hidden, use_bias=False, input_dim=self.latent_dim))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        # hidden layers\n",
    "        for layer in range(n_layers - 1):\n",
    "            if layer == n_layers - 2: # last layer with fewer neurons\n",
    "                n_hidden = n_hidden / 2\n",
    "\n",
    "            model.add(layers.Dense(n_hidden, use_bias=False))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "        # last layer\n",
    "        model.add(layers.Dense(self.data_dim))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        print(\"Generator Summary:\")\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        n_hidden= self.n_hidden\n",
    "        n_layers = self.n_layers\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        # first layer\n",
    "        model.add(layers.Dense(n_hidden, input_dim=self.data_dim))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        \n",
    "        # hidden layers\n",
    "        for layer in range(n_layers - 1):\n",
    "            if layer == n_layers - 2: # last layer with fewer neurons\n",
    "                n_hidden = n_hidden / 2\n",
    "\n",
    "            model.add(layers.Dense(n_hidden))\n",
    "            model.add(layers.LeakyReLU(alpha=0.2))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "\n",
    "        # last layer\n",
    "        model.add(layers.Dense(1))\n",
    "        \n",
    "        print(\"Discriminator Summary:\")\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        real_loss = self.cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        fake_loss = self.cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        total_loss = 0.5 * real_loss + 0.5 * fake_loss # TODO: add regularization\n",
    "        return total_loss\n",
    "    \n",
    "    def discriminator_acc(self, real_output, fake_output):\n",
    "        score_real = tf.sigmoid(real_output)\n",
    "        score_fake = tf.sigmoid(real_output)\n",
    "        acc_pos = tf.reduce_mean(tf.cast(score_real > 0.5, tf.float32))\n",
    "        acc_neg = tf.reduce_mean(tf.cast(score_fake < 0.5, tf.float32))\n",
    "        return acc_pos, acc_neg\n",
    "    \n",
    "    def generator_loss(self, fake_output):\n",
    "        cross_loss = self.cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "        return cross_loss\n",
    "\n",
    "    def correlation_loss(self, original, fake):\n",
    "        gcorr_real = np.array(self.mac * self.prev_gcorr_real + (1 - self.mac) * pd.DataFrame(original.numpy()).corr())\n",
    "        gcorr_fake = np.array(self.mac * self.prev_gcorr_fake + (1 - self.mac) * pd.DataFrame(fake.numpy()).corr())\n",
    "        corr_loss = tf.cast(tf.reduce_sum(tf.abs(gcorr_real - gcorr_fake)), tf.float32)    \n",
    "        # update for next iterations\n",
    "        self.prev_gcorr_real = gcorr_real\n",
    "        self.prev_gcorr_fake = gcorr_fake\n",
    "        return corr_loss\n",
    "    \n",
    "    # This annotation causes the function to be \"compiled\" and therefore run as graph\n",
    "    @tf.function \n",
    "    def train_step(self, data, noise, corr_loss, BATCH_SIZE):\n",
    "#         noise = tf.random.normal([BATCH_SIZE, self.data_dim])\n",
    "\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_data = self.generator(noise, training=True) # G(z)\n",
    "            real_output = self.discriminator(data, training=True) # D(x)\n",
    "            fake_output = self.discriminator(generated_data, training=True) # D(G(z))\n",
    "            \n",
    "            gen_loss = self.generator_loss(fake_output)\n",
    "            # generator loss + correlation loss\n",
    "            total_gen_loss = gen_loss + corr_loss # TODO: add regularization\n",
    "            disc_loss = self.discriminator_loss(real_output, fake_output)\n",
    "            \n",
    "            # accuracy\n",
    "            d_pos_acc, d_neg_acc = self.discriminator_acc(real_output, fake_output)\n",
    "            \n",
    "            \n",
    "        gradients_of_generator = gen_tape.gradient(total_gen_loss, \n",
    "                                                   self.generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, \n",
    "                                                        self.discriminator.trainable_variables)\n",
    "\n",
    "        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, \n",
    "                                                self.generator.trainable_variables))\n",
    "        self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, \n",
    "                                                    self.discriminator.trainable_variables))\n",
    "        return gen_loss, disc_loss, d_pos_acc, d_neg_acc\n",
    "        \n",
    "    def train(self, dataframe, EPOCHS, use_corr_loss=True, BATCH_SIZE=128, SAMPLE_INTERVAL=15):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # transform dataset\n",
    "        prepared_dataset = self.prepare_data(dataframe) \n",
    "                \n",
    "        # progress bar\n",
    "        pbar = tqdm(total=EPOCHS)\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            for data_batch in prepared_dataset:\n",
    "                noise = tf.random.normal([BATCH_SIZE, self.latent_dim])\n",
    "                generated = self.generator(noise, training=False)\n",
    "                \n",
    "                if use_corr_loss:\n",
    "                    corr_loss = self.correlation_loss(data_batch, generated)\n",
    "                else:\n",
    "                    corr_loss = 0\n",
    "                    \n",
    "                gen_loss, disc_loss, d_pos_acc, d_neg_acc = self.train_step(data_batch, \n",
    "                                                                            noise,\n",
    "                                                                            corr_loss,\n",
    "                                                                            BATCH_SIZE) #TODO: dont need batch_size in this function\n",
    "                \n",
    "            # Save the model every SAMPLE_INTERVAL epochs\n",
    "            if epoch % SAMPLE_INTERVAL == 0:\n",
    "                self.generate_data(epoch, BATCH_SIZE)\n",
    "                display.clear_output(wait=True)\n",
    "             #   checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            \n",
    "            # update losses and accuracy\n",
    "            self.loss_g.append(gen_loss.numpy())\n",
    "            self.loss_d.append(disc_loss.numpy())\n",
    "            self.acc_pos.append(d_pos_acc.numpy())\n",
    "            self.acc_neg.append(d_neg_acc.numpy())\n",
    "            \n",
    "#             print(\"D Loss: {0:.2f}, G Loss: {0:.2f}\".format(self.loss_d[-1], self.loss_g[-1]))   \n",
    "\n",
    "            # progress bar\n",
    "            pbar.update(1)\n",
    "            \n",
    "        # Generate after the final epoch\n",
    "        display.clear_output(wait=True)\n",
    "        self.generate_data(epoch, BATCH_SIZE * 5)\n",
    "    \n",
    "        pbar.close()\n",
    "    \n",
    "    def prepare_data(self, dataframe, BATCH_SIZE=128, cat_cols=[6, -1]):\n",
    "        \"\"\"Prepares, shuffles, and arrange data into batches.\n",
    "        Args:\n",
    "            dataframe(pandas.DataFrame): Dataset\n",
    "            BATCH_SIZE(integer): Size of each batch\n",
    "            cat_cols(list): Index of columns in the dataframe that are categorical\n",
    "        Returns:\n",
    "            X_train[numpy.ndarray]:  \n",
    "        \"\"\"\n",
    "        BUFFER_SIZE = dataframe.shape[0] * 2\n",
    "        self.columns = dataframe.columns.values\n",
    "        # copy the dataframe for later transformations\n",
    "        dataframe_copy = dataframe.copy()\n",
    "        # remove categorical columns\n",
    "        for cat_col in cat_cols:\n",
    "            dataframe_copy.drop(dataframe_copy.columns[cat_col], axis=1, inplace=True)\n",
    "            \n",
    "        # to numpy array and scale\n",
    "        X = np.array(dataframe_copy)\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # for the other categorical label\n",
    "        y_original = np.array(dataframe.iloc[:, cat_cols[0]])\n",
    "        # generate one-hot vector representation; e.g., 0 = [1 0 0 0 0], 1 = [0 1 0 0 0], etc.\n",
    "        one_hot = np.zeros((y_original.size, y_original.max() + 1)) \n",
    "        one_hot[np.arange(y_original.size),y_original] = 1\n",
    "        # adds noise to categorical columns\n",
    "        noise = tf.random.uniform(tf.shape(one_hot), minval=0, maxval=self.gamma)\n",
    "        noise_input = (one_hot + noise) / tf.reduce_sum(one_hot + noise, keepdims=True, axis=1)\n",
    "        # append back to X matrix\n",
    "        X_scaled = np.concatenate((X_scaled, noise_input), axis=1)\n",
    "        \n",
    "        # for target label \n",
    "        y_original = np.array(dataframe.iloc[:, cat_cols[1]])       \n",
    "        # re-map labels from {-2, -1, 0, 1, 2} to {0, 1, 2, 3, 4}; works better with _tanh_ activation\n",
    "        y_norm = y_original + 2\n",
    "        # generate one-hot vector representation; e.g., 0 = [1 0 0 0 0], 1 = [0 1 0 0 0], etc.\n",
    "        one_hot = np.zeros((y_norm.size, y_norm.max() + 1)) \n",
    "        one_hot[np.arange(y_norm.size),y_norm] = 1\n",
    "        # adds noise to categorical columns\n",
    "        noise = tf.random.uniform(tf.shape(one_hot), minval=0, maxval=self.gamma)\n",
    "        noise_input = (one_hot + noise) / tf.reduce_sum(one_hot + noise, keepdims=True, axis=1)\n",
    "        # append back to X matrix\n",
    "        X_scaled = np.concatenate((X_scaled, noise_input), axis=1)\n",
    "\n",
    "        # Batch and shuffle the data\n",
    "        X_train = tf.data.Dataset.from_tensor_slices(X_scaled).shuffle(BUFFER_SIZE, seed=self.seed).batch(BATCH_SIZE)\n",
    "        \n",
    "        return X_train\n",
    "    \n",
    "    def generate_data(self, epoch=1, BATCH_SIZE=128):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        noise = tf.random.normal([BATCH_SIZE, self.latent_dim])        \n",
    "        generated_x = self.generator(noise, training=False).numpy()\n",
    "\n",
    "        if self.display:\n",
    "            fig = plt.figure()\n",
    "            plt.hist(generated_x.numpy(), bins=40, density=True, histtype='bar')\n",
    "            plt.title(\"testing:\" + str(epoch))\n",
    "            plt.show()\n",
    "            fig.savefig(\"../output/GANtest/{}.png\".format(epoch))\n",
    "\n",
    "        # rescale back only the continous columns\n",
    "        generated_x_cont = generated_x[:,:8]\n",
    "        generated_x_cont = self.scaler.inverse_transform(generated_x_cont)\n",
    "\n",
    "        # get categorical variables\n",
    "        generated_x_cat_1 = generated_x[:,[8,9]]\n",
    "        generated_x_cat_2 = generated_x[:,10:]\n",
    "        # argmax to get the right label\n",
    "        generated_x_cat_1 = np.array([np.argmax(item) for item in generated_x_cat_1])\n",
    "        generated_x_cat_2 = np.array([np.argmax(item) for item in generated_x_cat_2])\n",
    "        # rescale the label with the correct offset\n",
    "        generated_x_cat_2 = generated_x_cat_2 - 2\n",
    "        \n",
    "        # insert columns back\n",
    "        generated_data = np.insert(generated_x_cont, 6, generated_x_cat_1, axis=1)\n",
    "        generated_data = np.insert(generated_data, generated_data.shape[1], generated_x_cat_2, axis=1)\n",
    "        generated_data = pd.DataFrame(generated_data, columns=self.columns)\n",
    "        \n",
    "        return generated_data\n",
    "    \n",
    "    def get_losses(self):\n",
    "        \"\"\"Return loses\n",
    "        Args: \n",
    "            None\n",
    "        Returns:    \n",
    "            loss_g\n",
    "            loss_d\n",
    "        \"\"\"\n",
    "        return self.loss_g, self.loss_d\n",
    "    \n",
    "    def get_accuracies(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            acc_pos\n",
    "            acc_neg\n",
    "        \"\"\"\n",
    "        return self.acc_pos, self.acc_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
