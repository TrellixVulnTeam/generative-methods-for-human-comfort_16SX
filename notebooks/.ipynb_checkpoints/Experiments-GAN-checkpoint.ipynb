{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T12:22:21.957051Z",
     "start_time": "2019-08-10T12:22:20.272461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from GAN.ipynb\n",
      "Importing Jupyter notebook from Utils.ipynb\n",
      "Importing Jupyter notebook from Functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Notebooks\n",
    "import nbimporter\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Functions Notebook\n",
    "from GAN import *\n",
    "from Utils import build_GAN, batch_build_GAN, corr_matrix\n",
    "from Functions import holisticsTrainTest\n",
    "from Functions import comfPMV, ppv, selectModelParameters, trainTest_tunedModel, buildTrainRF, buildTrainKNN\n",
    "from Functions import buildTrainSVM, buildTrainNB, buildMLP, getClfMetrics, saveModel\n",
    "\n",
    "# Pandas, matplotlib, pickle, seaborn\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from collections import Counter\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T12:22:21.962830Z",
     "start_time": "2019-08-10T12:22:21.958760Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run the pre-processing notebook first to make sure the pickle files exist\n",
    "df_feature1 = pd.read_pickle(\"df_feature1.pkl\")\n",
    "# df_feature2 = pd.read_pickle(\"df_feature2.pkl\")\n",
    "# df_feature3 = pd.read_pickle(\"df_feature3.pkl\")\n",
    "# df_feature4 = pd.read_pickle(\"df_feature4.pkl\")\n",
    "# df_feature5 = pd.read_pickle(\"df_feature5.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T12:22:21.986469Z",
     "start_time": "2019-08-10T12:22:21.964784Z"
    }
   },
   "outputs": [],
   "source": [
    "test_size_percentage = 0.2 # for CV within train split\n",
    "train_test_split = 0.7 # for main train/validation split\n",
    "\n",
    "user_split = True #split train and test user-based or completely stratfied\n",
    "\n",
    "use_heuristics_participants = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T12:22:22.039619Z",
     "start_time": "2019-08-10T12:22:21.988067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Participants: [1, 2, 4, 5, 6, 7, 8, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 29, 30, 31, 32, 33, 34, 35, 40, 41, 42, 43, 44, 45, 46, 49, 50, 51, 52, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80]\n",
      "Total number of complete participants: 65\n"
     ]
    }
   ],
   "source": [
    "df_aux_65 = pd.read_csv(\"../data/TCS_65_participants_outsideData.csv\", delimiter = \",\")\n",
    "\n",
    "list_complete_participants = list(df_aux_65['Participant_No'].unique())\n",
    "\n",
    "if use_heuristics_participants:\n",
    "    list_complete_participants.append(10)\n",
    "    list_complete_participants.append(26)\n",
    "    list_complete_participants.append(28)\n",
    "    list_complete_participants.append(36)\n",
    "    list_complete_participants.append(37)\n",
    "    list_complete_participants.append(39)\n",
    "    list_complete_participants.append(47)\n",
    "    list_complete_participants.append(48)\n",
    "    list_complete_participants.append(53)\n",
    "\n",
    "num_complete_participants = len(list_complete_participants)\n",
    "print(\"Complete Participants: {}\".format(list_complete_participants))\n",
    "print(\"Total number of complete participants: {}\".format(num_complete_participants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T13:48:26.079427Z",
     "start_time": "2019-08-10T13:48:26.068045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num participants in test set: \n",
      "Testing on participants:\n",
      "[ 4  7  8 21 25 29 40 41 43 57 58 59 61 64 66 68 70 71 73 80]\n",
      "Total number of instances: 2067\n",
      " 0    1152\n",
      "-1     452\n",
      "-2     217\n",
      " 1     198\n",
      " 2      48\n",
      "Name: Discrete Thermal Comfort_TA, dtype: int64\n",
      "Number of training instances: 1508\n",
      "Number of testing (validation) instances: 559\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_feature1_train, df_feature1_test, df_feature1_train_binary, df_feature1_test_binary = holisticsTrainTest(df_feature1, \n",
    "                                                                                                            list_complete_participants,\n",
    "                                                                                                            user_split = user_split,\n",
    "                                                                                                            train_test_split = train_test_split)\n",
    "\n",
    "# df_feature2_train, df_feature2_test, df_feature2_train_binary, df_feature2_test_binary  = holisticsTrainTest(df_feature2, \n",
    "#                                                                                                              list_complete_participants,\n",
    "#                                                                                                             user_split = user_split,\n",
    "#                                                                                                             train_test_split=train_test_split)\n",
    "\n",
    "# df_feature3_train, df_feature3_test, df_feature3_train_binary, df_feature3_test_binary = holisticsTrainTest(df_feature3,\n",
    "#                                                                                                            list_complete_participants,\n",
    "#                                                                                                            user_split = user_split,\n",
    "#                                                                                                            train_test_split=train_test_split)\n",
    "\n",
    "# df_feature4_train, df_feature4_test, df_feature4_train_binary, df_feature4_test_binary = holisticsTrainTest(df_feature4,\n",
    "#                                                                                                            list_complete_participants,\n",
    "#                                                                                                            user_split = user_split,\n",
    "#                                                                                                            train_test_split=train_test_split)\n",
    "\n",
    "# df_feature5_train, df_feature5_test, df_feature5_train_binary, df_feature5_test_binary = holisticsTrainTest(df_feature5,\n",
    "#                                                                                                             list_complete_participants,\n",
    "#                                                                                                            user_split = user_split,\n",
    "#                                                                                                            train_test_split=train_test_split)\n",
    "\n",
    "print(\"Total number of instances: {}\".format(df_feature1.shape[0]))\n",
    "print(df_feature1['Discrete Thermal Comfort_TA'].value_counts())\n",
    "print(\"Number of training instances: {}\".format(df_feature1_train.shape[0]))\n",
    "print(\"Number of testing (validation) instances: {}\".format(df_feature1_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T13:48:35.186530Z",
     "start_time": "2019-08-10T13:48:35.180534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    818\n",
      "-1    303\n",
      "-2    172\n",
      " 1    171\n",
      " 2     44\n",
      "Name: Discrete Thermal Comfort_TA, dtype: int64\n",
      " 0    334\n",
      "-1    149\n",
      "-2     45\n",
      " 1     27\n",
      " 2      4\n",
      "Name: Discrete Thermal Comfort_TA, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_feature1_train['Discrete Thermal Comfort_TA'].value_counts())\n",
    "print(df_feature1_test['Discrete Thermal Comfort_TA'].value_counts())\n",
    "# df_feature1_train.to_csv('df_feature1_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:28:06.052686Z",
     "start_time": "2019-08-10T15:28:06.048598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818\n",
      "606\n",
      "516\n",
      "513\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "# needed datapoints per label\n",
    "needed_0 = 818\n",
    "needed_minus1 = 303 * 2\n",
    "needed_minus2 = 172 * 3\n",
    "needed_1 = 171 * 3\n",
    "needed_2 = 44 * 3\n",
    "\n",
    "print(needed_0)\n",
    "print(needed_minus1)\n",
    "print(needed_minus2)\n",
    "print(needed_1)\n",
    "print(needed_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occutherm thermal comfort models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T12:24:37.915174Z",
     "start_time": "2019-08-10T12:22:22.078963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Expected accuracy (f1 micro) based on Cross-Validation:  0.4933616000281488\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "Accuracy (f1 micro) on test set:  0.5463576158940397\n",
      "F1 micro on test set:  0.5463576158940397\n",
      "F1 macro on test set:  0.4253646149609908\n",
      "Confusion Matrix: \n",
      "[[ 19  11   4   0   0]\n",
      " [ 21  23  15   2   0]\n",
      " [ 11  25 111  10   7]\n",
      " [  0   1  17   7   9]\n",
      " [  0   0   3   1   5]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.37      0.56      0.45        34\n",
      "          -1       0.38      0.38      0.38        61\n",
      "           0       0.74      0.68      0.71       164\n",
      "           1       0.35      0.21      0.26        34\n",
      "           2       0.24      0.56      0.33         9\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       302\n",
      "   macro avg       0.42      0.47      0.43       302\n",
      "weighted avg       0.57      0.55      0.55       302\n",
      "\n",
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Number of folds: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'algorithm': 'brute', 'metric': 'seuclidean', 'n_neighbors': 14, 'weights': 'distance'}\n",
      "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='seuclidean',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=14, p=2,\n",
      "           weights='distance')\n",
      "Accuracy (f1 micro) on test set:  0.6821192052980133\n",
      "F1 micro on test set:  0.6821192052980133\n",
      "F1 macro on test set:  0.5928229602002539\n",
      "Confusion Matrix: \n",
      "[[ 16  14   4   0   0]\n",
      " [  8  31  21   1   0]\n",
      " [  1  16 140   7   0]\n",
      " [  0   1  18  15   0]\n",
      " [  0   0   2   3   4]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.64      0.47      0.54        34\n",
      "          -1       0.50      0.51      0.50        61\n",
      "           0       0.76      0.85      0.80       164\n",
      "           1       0.58      0.44      0.50        34\n",
      "           2       1.00      0.44      0.62         9\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       302\n",
      "   macro avg       0.69      0.54      0.59       302\n",
      "weighted avg       0.68      0.68      0.67       302\n",
      "\n",
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Number of folds: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'C': 1000, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=100, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy (f1 micro) on test set:  0.6688741721854304\n",
      "F1 micro on test set:  0.6688741721854304\n",
      "F1 macro on test set:  0.5744850016291645\n",
      "Confusion Matrix: \n",
      "[[ 18  12   4   0   0]\n",
      " [ 16  35   9   1   0]\n",
      " [  8  21 123  10   2]\n",
      " [  0   1   8  22   3]\n",
      " [  0   0   1   4   4]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.43      0.53      0.47        34\n",
      "          -1       0.51      0.57      0.54        61\n",
      "           0       0.85      0.75      0.80       164\n",
      "           1       0.59      0.65      0.62        34\n",
      "           2       0.44      0.44      0.44         9\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       302\n",
      "   macro avg       0.56      0.59      0.57       302\n",
      "weighted avg       0.69      0.67      0.68       302\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.5885509838998211\n",
      "F1 micro on test set:  0.5885509838998211\n",
      "F1 macro on test set:  0.33732679443176394\n",
      "Confusion Matrix: \n",
      "[[  8  17  20   0   0]\n",
      " [ 30  66  51   2   0]\n",
      " [ 10  41 242  41   0]\n",
      " [  0   0  14  13   0]\n",
      " [  0   0   0   4   0]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.17      0.18      0.17        45\n",
      "          -1       0.53      0.44      0.48       149\n",
      "           0       0.74      0.72      0.73       334\n",
      "           1       0.22      0.48      0.30        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.59      0.59      0.59       559\n",
      "   macro avg       0.33      0.37      0.34       559\n",
      "weighted avg       0.61      0.59      0.59       559\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.5974955277280859\n",
      "F1 micro on test set:  0.5974955277280859\n",
      "F1 macro on test set:  0.1496080627099664\n",
      "Confusion Matrix: \n",
      "[[  0   0  45   0   0]\n",
      " [  0   0 149   0   0]\n",
      " [  0   0 334   0   0]\n",
      " [  0   0  27   0   0]\n",
      " [  0   0   4   0   0]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.00      0.00      0.00        45\n",
      "          -1       0.00      0.00      0.00       149\n",
      "           0       0.60      1.00      0.75       334\n",
      "           1       0.00      0.00      0.00        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.60      0.60      0.60       559\n",
      "   macro avg       0.12      0.20      0.15       559\n",
      "weighted avg       0.36      0.60      0.45       559\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.5974955277280859\n",
      "F1 micro on test set:  0.5974955277280859\n",
      "F1 macro on test set:  0.1496080627099664\n",
      "Confusion Matrix: \n",
      "[[  0   0  45   0   0]\n",
      " [  0   0 149   0   0]\n",
      " [  0   0 334   0   0]\n",
      " [  0   0  27   0   0]\n",
      " [  0   0   4   0   0]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.00      0.00      0.00        45\n",
      "          -1       0.00      0.00      0.00       149\n",
      "           0       0.60      1.00      0.75       334\n",
      "           1       0.00      0.00      0.00        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.60      0.60      0.60       559\n",
      "   macro avg       0.12      0.20      0.15       559\n",
      "weighted avg       0.36      0.60      0.45       559\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# load best models NB and KNN\n",
    "acc_nb_1_train, nb_optimal_1 = buildTrainNB(df_feature1_train, test_size_percentage=test_size_percentage)\n",
    "acc_knn_1_train, knn_optimal_1 = buildTrainKNN(df_feature1_train, test_size_percentage=test_size_percentage)\n",
    "acc_svm_1_train, svm_optimal_1 = buildTrainSVM(df_feature1_train, test_size_percentage=test_size_percentage)\n",
    "# using the optimal model. re-train in whole train split and test in unseen test split\n",
    "acc_holistic_knn_1, _ = trainTest_tunedModel(df_feature1_test, knn_optimal_1)\n",
    "acc_holistic_nb_1, _ = trainTest_tunedModel(df_feature1_test, nb_optimal_1)\n",
    "acc_holistic_svm_1, _= trainTest_tunedModel(df_feature1_test, svm_optimal_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T12:24:44.523594Z",
     "start_time": "2019-08-10T12:24:37.916776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. -2. -1.  1.  2.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fcd68077780>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 10 # 1000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# testing\n",
    "gan = GAN(data_dim=15) # calculated based on new expanded categorical features and continous\n",
    "\n",
    "gan.train(df_feature1_train, EPOCHS=EPOCHS)\n",
    "\n",
    "gen_data = gan.generate_data(epoch=1, BATCH_SIZE=BATCH_SIZE)\n",
    "\n",
    "loss_g, loss_d = gan.get_losses()\n",
    "\n",
    "print(gen_data['Discrete Thermal Comfort_TA'].unique())\n",
    "plt.figure()\n",
    "plt.plot(loss_d, label='Disc')\n",
    "plt.plot(loss_g, label='Gen')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T12:24:44.527111Z",
     "start_time": "2019-08-10T12:24:44.524984Z"
    }
   },
   "outputs": [],
   "source": [
    "find_best_arch = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T12:24:44.543624Z",
     "start_time": "2019-08-10T12:24:44.528362Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if find_best_arch:\n",
    "    # TANH\n",
    "    # parameters\n",
    "    batch_parameters = {\n",
    "        'BATCH_SIZE' : [10, 50, 100, 200],\n",
    "        'n_hidden' : [50, 100, 150, 200, 250],\n",
    "        'n_layers': [2, 3],\n",
    "        'use_corr_loss': [True], \n",
    "        'EPOCHS': [1000],\n",
    "        'lr': [0.0001]\n",
    "    }\n",
    "    # print(df_feature1_train)\n",
    "    # build GAN\n",
    "    gen_df_list, loss_g_list, loss_d_list, acc_pos_list, acc_neg_list, parameters_list, corr_diff_list, corr_reduced_list = batch_build_GAN(df_feature1_train, batch_parameters)\n",
    "    # 21 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T12:24:44.553786Z",
     "start_time": "2019-08-10T12:24:44.544861Z"
    }
   },
   "outputs": [],
   "source": [
    "# best_arch = np.argmin(corr_reduced_list)\n",
    "# print(best_arch)\n",
    "# print(len(parameters_list))\n",
    "# print(parameters_list[best_arch])\n",
    "\n",
    "# print(parameters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T12:24:44.564890Z",
     "start_time": "2019-08-10T12:24:44.554934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n{'use_corr_loss': True, \\n'BATCH_SIZE': 10, \\n'n_hidden': 50, \\n'n_layers': 3, \\n'EPOCHS': 1000, \\n'lr': 0.0001}\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "{'use_corr_loss': True, \n",
    "'BATCH_SIZE': 10, \n",
    "'n_hidden': 50, \n",
    "'n_layers': 3, \n",
    "'EPOCHS': 1000, \n",
    "'lr': 0.0001}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T12:28:51.560294Z",
     "start_time": "2019-08-10T12:24:44.566012Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# with corr\n",
    "EPOCHS = 1000 # 1000\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# testing\n",
    "gan_corr = GAN(data_dim=15,\n",
    "          n_hidden=150,\n",
    "          n_layers=3) # calculated based on new expanded categorical features and continous\n",
    "\n",
    "gan_corr.train(df_feature1_train, EPOCHS=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T12:28:51.566042Z",
     "start_time": "2019-08-10T12:28:51.561752Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_needed_label(num_samples, label, gan):\n",
    "    synth_df = pd.DataFrame(columns=df_feature1.columns.values[1:])\n",
    "    count = 0\n",
    "    finish_loop = False\n",
    "    # generate as many sampels of label 0 as needed\n",
    "    while True:\n",
    "        if finish_loop:\n",
    "            break\n",
    "\n",
    "        # sample a bunch of points\n",
    "        curr_df = gan.generate_data(epoch=1, BATCH_SIZE=1000)\n",
    "        # iterate through all\n",
    "        for index, row in curr_df.iterrows():\n",
    "            if row['Discrete Thermal Comfort_TA'] == label:\n",
    "                synth_df = synth_df.append(row)\n",
    "                count += 1\n",
    "                print(count)\n",
    "                if count == num_samples:\n",
    "                    finish_loop = True\n",
    "                    break\n",
    "    return synth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T13:43:45.460677Z",
     "start_time": "2019-08-10T13:43:44.399979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples needed: 818\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples needed: {}\".format(needed_0))\n",
    "synth_0 = sample_needed_label(needed_0, 0, gan_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:28:21.366258Z",
     "start_time": "2019-08-10T15:28:20.287417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples needed: 606\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples needed: {}\".format(needed_minus1))\n",
    "synth_minus1 = sample_needed_label(needed_minus1, -1, gan_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:32:48.429149Z",
     "start_time": "2019-08-10T15:28:24.811362Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples needed: 516\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples needed: {}\".format(needed_minus2))\n",
    "synth_minus2 = sample_needed_label(needed_minus2, -2, gan_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:32:52.185088Z",
     "start_time": "2019-08-10T15:32:51.421150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples needed: 513\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples needed: {}\".format(needed_1))\n",
    "synth_1 = sample_needed_label(needed_1, 1, gan_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:40:20.545387Z",
     "start_time": "2019-08-10T15:32:55.083812Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples needed: 132\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples needed: {}\".format(needed_2))\n",
    "synth_2 = sample_needed_label(needed_2, 2, gan_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:40:23.447453Z",
     "start_time": "2019-08-10T15:40:23.416812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0    606\n",
      "-2.0    516\n",
      " 1.0    513\n",
      " 2.0    132\n",
      "Name: Discrete Thermal Comfort_TA, dtype: int64\n",
      "-1.0    909\n",
      " 0.0    818\n",
      "-2.0    688\n",
      " 1.0    684\n",
      " 2.0    176\n",
      "Name: Discrete Thermal Comfort_TA, dtype: int64\n",
      "      Temperature (Fahrenheit)  SkinTemperature  ClothingInsulation  \\\n",
      "0                    66.663147        97.163948            0.949598   \n",
      "1                    67.238152        89.123779            0.711082   \n",
      "2                    64.172806        91.701340            0.555487   \n",
      "3                    68.483086        88.953461            0.585449   \n",
      "4                    66.311096        81.253853            0.461899   \n",
      "5                    66.154556        80.969681            0.432399   \n",
      "6                    64.777222        94.111298            0.325522   \n",
      "7                    69.815903        83.549950            0.778107   \n",
      "8                    61.611633        87.612579            0.268961   \n",
      "9                    70.676453       102.550507            0.527346   \n",
      "10                   66.685707        88.099419            0.811116   \n",
      "11                   65.781380        84.700874            0.528575   \n",
      "12                   63.639893        86.966919            0.721070   \n",
      "13                   61.011059        98.976120            0.630946   \n",
      "14                   64.786263       100.014992            0.882802   \n",
      "15                   66.453194        91.446175            0.707591   \n",
      "16                   63.875359        94.969917            0.856511   \n",
      "17                   68.028679        79.664757            0.812001   \n",
      "18                   67.174316        92.636742            0.669958   \n",
      "19                   65.476250        84.638969            0.734104   \n",
      "20                   62.492657        86.136566            0.801279   \n",
      "21                   63.176361        91.796776            0.327362   \n",
      "22                   66.292526        98.266953            0.552410   \n",
      "23                   68.180084        87.754143            0.399227   \n",
      "24                   77.450409       102.313210            0.817751   \n",
      "25                   67.811981        96.908554            0.681480   \n",
      "26                   69.624443        81.115204            0.534187   \n",
      "27                   62.574581        89.440086            0.736862   \n",
      "28                   65.472725        83.957069            0.739413   \n",
      "29                   67.017899        77.307678            0.375453   \n",
      "...                        ...              ...                 ...   \n",
      "3245                 78.800003        90.635001            0.490000   \n",
      "3246                 64.400002        81.983839            0.650000   \n",
      "3247                 64.599998        80.755821            0.490000   \n",
      "3248                 65.599998        79.692337            0.650000   \n",
      "3249                 78.099998        88.514000            0.360000   \n",
      "3250                 76.099998        84.890000            1.070000   \n",
      "3251                 77.110001        89.821999            0.490000   \n",
      "3252                 77.540001        86.378000            0.490000   \n",
      "3253                 64.699997        85.382000            0.680000   \n",
      "3254                 64.000000        79.256000            0.580000   \n",
      "3255                 67.800003        82.544001            0.400000   \n",
      "3256                 80.099998        84.002001            0.580000   \n",
      "3257                 66.699997        98.631219            0.650000   \n",
      "3258                 70.199997        92.336001            0.710000   \n",
      "3259                 64.199997        86.696000            0.400000   \n",
      "3260                 64.400002        87.278001            0.570000   \n",
      "3261                 66.000000        80.510001            0.480000   \n",
      "3262                 77.349998        87.398001            0.320000   \n",
      "3263                 64.900002        81.139999            1.020000   \n",
      "3264                 74.900002        91.832000            0.400000   \n",
      "3265                 67.800003        89.408000            0.770000   \n",
      "3266                 65.599998        82.892001            0.680000   \n",
      "3267                 78.800003        86.323999            0.410000   \n",
      "3268                 66.000000        82.184099            0.710000   \n",
      "3269                 75.400002        86.120000            0.580000   \n",
      "3270                 71.199997        85.112947            0.490000   \n",
      "3271                 61.130001        84.128001            0.490000   \n",
      "3272                 63.799999        84.566000            0.660000   \n",
      "3273                 74.410004        86.306000            0.570000   \n",
      "3274                 77.599998        80.211576            0.490000   \n",
      "\n",
      "      Height(cm)  Shoulder Circumference(cm)  Weight(lbs)  Gender  \\\n",
      "0     166.894821                  106.158890   130.734222     0.0   \n",
      "1     174.672882                   96.983116   109.013542     0.0   \n",
      "2     174.208633                  115.356224   144.756149     1.0   \n",
      "3     181.848465                   97.102409   129.641602     0.0   \n",
      "4     179.020615                  103.394241   144.158905     1.0   \n",
      "5     178.031204                  123.000183   180.723007     1.0   \n",
      "6     159.885193                   99.772560   135.837677     0.0   \n",
      "7     166.829803                  107.529549   133.403198     0.0   \n",
      "8     165.464294                   90.996368    94.750305     0.0   \n",
      "9     174.428879                  100.249855   136.763351     0.0   \n",
      "10    180.092407                  124.282288   174.638077     1.0   \n",
      "11    152.774185                  108.986786   155.592422     0.0   \n",
      "12    173.018921                  121.376289   153.578384     1.0   \n",
      "13    148.366287                  105.079880   132.707428     0.0   \n",
      "14    162.612076                   99.244667   111.077515     0.0   \n",
      "15    170.555237                  101.062080   135.127243     0.0   \n",
      "16    162.625443                   99.759346   134.333298     0.0   \n",
      "17    176.588837                  116.499977   148.673355     1.0   \n",
      "18    178.407532                  119.032547   152.190094     1.0   \n",
      "19    180.206375                  116.730019   155.059219     1.0   \n",
      "20    167.426788                  104.384392   137.846603     0.0   \n",
      "21    169.987579                  108.502907   148.251816     0.0   \n",
      "22    177.524658                   97.008057   123.924118     0.0   \n",
      "23    180.640518                  122.013885   221.830231     1.0   \n",
      "24    150.373566                  103.040535   127.293823     0.0   \n",
      "25    172.489410                  101.516022   135.822281     0.0   \n",
      "26    179.262222                  100.035522   135.557602     0.0   \n",
      "27    161.817612                  106.445236   125.452393     0.0   \n",
      "28    166.509766                  103.439415   139.371246     0.0   \n",
      "29    166.051880                  108.617035   190.340515     0.0   \n",
      "...          ...                         ...          ...     ...   \n",
      "3245  162.000000                  101.000000   120.000000     0.0   \n",
      "3246  185.000000                  104.000000   115.000000     1.0   \n",
      "3247  169.000000                  108.500000   141.200000     0.0   \n",
      "3248  185.000000                  104.000000   115.000000     1.0   \n",
      "3249  170.200000                  104.200000   184.300000     0.0   \n",
      "3250  183.000000                  124.000000   182.000000     1.0   \n",
      "3251  155.400000                  102.400000   117.200000     0.0   \n",
      "3252  163.300000                  106.900000   166.200000     0.0   \n",
      "3253  173.000000                  101.000000   140.000000     0.0   \n",
      "3254  156.000000                   89.500000   111.000000     0.0   \n",
      "3255  157.500000                   92.000000    90.000000     0.0   \n",
      "3256  156.000000                   89.500000   111.000000     0.0   \n",
      "3257  185.000000                  104.000000   115.000000     1.0   \n",
      "3258  164.000000                  106.000000   159.400000     1.0   \n",
      "3259  189.000000                  130.000000   236.600000     1.0   \n",
      "3260  165.000000                  108.000000   146.000000     1.0   \n",
      "3261  177.000000                   94.500000   130.000000     0.0   \n",
      "3262  188.100000                  118.200000   185.700000     1.0   \n",
      "3263  164.000000                  101.600000   125.000000     0.0   \n",
      "3264  189.000000                  130.000000   236.600000     1.0   \n",
      "3265  158.000000                   98.000000   120.000000     0.0   \n",
      "3266  180.000000                  105.000000   143.300000     1.0   \n",
      "3267  176.700000                  106.200000   161.500000     1.0   \n",
      "3268  183.000000                  132.000000   170.000000     1.0   \n",
      "3269  175.000000                  112.000000   158.500000     1.0   \n",
      "3270  177.000000                  109.000000   145.000000     1.0   \n",
      "3271  155.400000                  102.400000   117.200000     0.0   \n",
      "3272  182.300000                  118.700000   167.600000     1.0   \n",
      "3273  188.100000                  118.200000   185.700000     1.0   \n",
      "3274  177.000000                  109.000000   145.000000     1.0   \n",
      "\n",
      "      Temperature_outside  Humidity_outside  Discrete Thermal Comfort_TA  \n",
      "0               29.832623         68.864960                         -1.0  \n",
      "1               48.405701         49.266506                         -1.0  \n",
      "2               35.385986         52.928055                         -1.0  \n",
      "3               55.963390         57.311100                         -1.0  \n",
      "4               30.776283         69.676582                         -1.0  \n",
      "5               32.140011         72.269142                         -1.0  \n",
      "6               48.172729         70.023460                         -1.0  \n",
      "7               39.891029         56.908035                         -1.0  \n",
      "8               45.833992         58.163425                         -1.0  \n",
      "9               47.029858         65.612587                         -1.0  \n",
      "10              21.888605         61.565166                         -1.0  \n",
      "11              74.376343         81.394867                         -1.0  \n",
      "12              40.624783         49.160225                         -1.0  \n",
      "13              61.742626         82.546738                         -1.0  \n",
      "14              42.479713         66.015060                         -1.0  \n",
      "15              30.167543         75.211815                         -1.0  \n",
      "16              34.610477         82.067207                         -1.0  \n",
      "17              41.412273         92.044571                         -1.0  \n",
      "18              29.742342         55.184467                         -1.0  \n",
      "19              27.980129         68.199471                         -1.0  \n",
      "20              47.898540         79.039413                         -1.0  \n",
      "21              37.071358         76.769470                         -1.0  \n",
      "22              38.362236         61.207874                         -1.0  \n",
      "23              69.872681         65.107277                         -1.0  \n",
      "24              47.396725         84.106323                         -1.0  \n",
      "25              48.672035         63.638260                         -1.0  \n",
      "26              46.161423         60.378052                         -1.0  \n",
      "27              47.271839         53.702290                         -1.0  \n",
      "28              59.531761         74.389709                         -1.0  \n",
      "29              58.596111         66.261688                         -1.0  \n",
      "...                   ...               ...                          ...  \n",
      "3245            52.070000         60.400002                          0.0  \n",
      "3246            54.520000         60.400002                          0.0  \n",
      "3247            46.750000         62.599998                          0.0  \n",
      "3248            53.520000         60.099998                          0.0  \n",
      "3249            87.570000         53.000000                          1.0  \n",
      "3250            20.670000         70.000000                          0.0  \n",
      "3251            89.550003         55.200001                          0.0  \n",
      "3252            79.959999         73.400002                          0.0  \n",
      "3253            42.799999         86.500000                         -2.0  \n",
      "3254            42.889999         67.900002                         -2.0  \n",
      "3255            32.240002         64.300003                         -2.0  \n",
      "3256            41.919998         72.699997                          2.0  \n",
      "3257            54.650002         61.500000                          0.0  \n",
      "3258            39.080002         59.200001                          0.0  \n",
      "3259            52.549999         76.099998                          0.0  \n",
      "3260            58.630001         71.599998                         -1.0  \n",
      "3261            70.239998         47.799999                          0.0  \n",
      "3262            80.139999         85.800003                          0.0  \n",
      "3263            29.610001         66.300003                         -1.0  \n",
      "3264            47.419998         81.800003                          0.0  \n",
      "3265            29.350000         75.500000                         -1.0  \n",
      "3266            10.510000         70.099998                         -1.0  \n",
      "3267            69.260002         71.199997                          1.0  \n",
      "3268            53.599998         34.400002                         -1.0  \n",
      "3269            35.540001         65.000000                          0.0  \n",
      "3270            44.130001         72.000000                          0.0  \n",
      "3271            88.580002         56.200001                         -1.0  \n",
      "3272            66.339996         60.700001                         -2.0  \n",
      "3273            80.139999         88.400002                          0.0  \n",
      "3274            44.720001         72.800003                          1.0  \n",
      "\n",
      "[3275 rows x 10 columns]\n",
      "      Temperature (Fahrenheit)  SkinTemperature  ClothingInsulation  \\\n",
      "0                        66.66            97.16                0.95   \n",
      "1                        67.24            89.12                0.71   \n",
      "2                        64.17            91.70                0.56   \n",
      "3                        68.48            88.95                0.59   \n",
      "4                        66.31            81.25                0.46   \n",
      "5                        66.15            80.97                0.43   \n",
      "6                        64.78            94.11                0.33   \n",
      "7                        69.82            83.55                0.78   \n",
      "8                        61.61            87.61                0.27   \n",
      "9                        70.68           102.55                0.53   \n",
      "10                       66.69            88.10                0.81   \n",
      "11                       65.78            84.70                0.53   \n",
      "12                       63.64            86.97                0.72   \n",
      "13                       61.01            98.98                0.63   \n",
      "14                       64.79           100.01                0.88   \n",
      "15                       66.45            91.45                0.71   \n",
      "16                       63.88            94.97                0.86   \n",
      "17                       68.03            79.66                0.81   \n",
      "18                       67.17            92.64                0.67   \n",
      "19                       65.48            84.64                0.73   \n",
      "20                       62.49            86.14                0.80   \n",
      "21                       63.18            91.80                0.33   \n",
      "22                       66.29            98.27                0.55   \n",
      "23                       68.18            87.75                0.40   \n",
      "24                       77.45           102.31                0.82   \n",
      "25                       67.81            96.91                0.68   \n",
      "26                       69.62            81.12                0.53   \n",
      "27                       62.57            89.44                0.74   \n",
      "28                       65.47            83.96                0.74   \n",
      "29                       67.02            77.31                0.38   \n",
      "...                        ...              ...                 ...   \n",
      "3245                     78.80            90.64                0.49   \n",
      "3246                     64.40            81.98                0.65   \n",
      "3247                     64.60            80.76                0.49   \n",
      "3248                     65.60            79.69                0.65   \n",
      "3249                     78.10            88.51                0.36   \n",
      "3250                     76.10            84.89                1.07   \n",
      "3251                     77.11            89.82                0.49   \n",
      "3252                     77.54            86.38                0.49   \n",
      "3253                     64.70            85.38                0.68   \n",
      "3254                     64.00            79.26                0.58   \n",
      "3255                     67.80            82.54                0.40   \n",
      "3256                     80.10            84.00                0.58   \n",
      "3257                     66.70            98.63                0.65   \n",
      "3258                     70.20            92.34                0.71   \n",
      "3259                     64.20            86.70                0.40   \n",
      "3260                     64.40            87.28                0.57   \n",
      "3261                     66.00            80.51                0.48   \n",
      "3262                     77.35            87.40                0.32   \n",
      "3263                     64.90            81.14                1.02   \n",
      "3264                     74.90            91.83                0.40   \n",
      "3265                     67.80            89.41                0.77   \n",
      "3266                     65.60            82.89                0.68   \n",
      "3267                     78.80            86.32                0.41   \n",
      "3268                     66.00            82.18                0.71   \n",
      "3269                     75.40            86.12                0.58   \n",
      "3270                     71.20            85.11                0.49   \n",
      "3271                     61.13            84.13                0.49   \n",
      "3272                     63.80            84.57                0.66   \n",
      "3273                     74.41            86.31                0.57   \n",
      "3274                     77.60            80.21                0.49   \n",
      "\n",
      "      Height(cm)  Shoulder Circumference(cm)  Weight(lbs)  Gender  \\\n",
      "0         166.89                      106.16       130.73     0.0   \n",
      "1         174.67                       96.98       109.01     0.0   \n",
      "2         174.21                      115.36       144.76     1.0   \n",
      "3         181.85                       97.10       129.64     0.0   \n",
      "4         179.02                      103.39       144.16     1.0   \n",
      "5         178.03                      123.00       180.72     1.0   \n",
      "6         159.89                       99.77       135.84     0.0   \n",
      "7         166.83                      107.53       133.40     0.0   \n",
      "8         165.46                       91.00        94.75     0.0   \n",
      "9         174.43                      100.25       136.76     0.0   \n",
      "10        180.09                      124.28       174.64     1.0   \n",
      "11        152.77                      108.99       155.59     0.0   \n",
      "12        173.02                      121.38       153.58     1.0   \n",
      "13        148.37                      105.08       132.71     0.0   \n",
      "14        162.61                       99.24       111.08     0.0   \n",
      "15        170.56                      101.06       135.13     0.0   \n",
      "16        162.63                       99.76       134.33     0.0   \n",
      "17        176.59                      116.50       148.67     1.0   \n",
      "18        178.41                      119.03       152.19     1.0   \n",
      "19        180.21                      116.73       155.06     1.0   \n",
      "20        167.43                      104.38       137.85     0.0   \n",
      "21        169.99                      108.50       148.25     0.0   \n",
      "22        177.52                       97.01       123.92     0.0   \n",
      "23        180.64                      122.01       221.83     1.0   \n",
      "24        150.37                      103.04       127.29     0.0   \n",
      "25        172.49                      101.52       135.82     0.0   \n",
      "26        179.26                      100.04       135.56     0.0   \n",
      "27        161.82                      106.45       125.45     0.0   \n",
      "28        166.51                      103.44       139.37     0.0   \n",
      "29        166.05                      108.62       190.34     0.0   \n",
      "...          ...                         ...          ...     ...   \n",
      "3245      162.00                      101.00       120.00     0.0   \n",
      "3246      185.00                      104.00       115.00     1.0   \n",
      "3247      169.00                      108.50       141.20     0.0   \n",
      "3248      185.00                      104.00       115.00     1.0   \n",
      "3249      170.20                      104.20       184.30     0.0   \n",
      "3250      183.00                      124.00       182.00     1.0   \n",
      "3251      155.40                      102.40       117.20     0.0   \n",
      "3252      163.30                      106.90       166.20     0.0   \n",
      "3253      173.00                      101.00       140.00     0.0   \n",
      "3254      156.00                       89.50       111.00     0.0   \n",
      "3255      157.50                       92.00        90.00     0.0   \n",
      "3256      156.00                       89.50       111.00     0.0   \n",
      "3257      185.00                      104.00       115.00     1.0   \n",
      "3258      164.00                      106.00       159.40     1.0   \n",
      "3259      189.00                      130.00       236.60     1.0   \n",
      "3260      165.00                      108.00       146.00     1.0   \n",
      "3261      177.00                       94.50       130.00     0.0   \n",
      "3262      188.10                      118.20       185.70     1.0   \n",
      "3263      164.00                      101.60       125.00     0.0   \n",
      "3264      189.00                      130.00       236.60     1.0   \n",
      "3265      158.00                       98.00       120.00     0.0   \n",
      "3266      180.00                      105.00       143.30     1.0   \n",
      "3267      176.70                      106.20       161.50     1.0   \n",
      "3268      183.00                      132.00       170.00     1.0   \n",
      "3269      175.00                      112.00       158.50     1.0   \n",
      "3270      177.00                      109.00       145.00     1.0   \n",
      "3271      155.40                      102.40       117.20     0.0   \n",
      "3272      182.30                      118.70       167.60     1.0   \n",
      "3273      188.10                      118.20       185.70     1.0   \n",
      "3274      177.00                      109.00       145.00     1.0   \n",
      "\n",
      "      Temperature_outside  Humidity_outside  Discrete Thermal Comfort_TA  \n",
      "0                   29.83             68.86                         -1.0  \n",
      "1                   48.41             49.27                         -1.0  \n",
      "2                   35.39             52.93                         -1.0  \n",
      "3                   55.96             57.31                         -1.0  \n",
      "4                   30.78             69.68                         -1.0  \n",
      "5                   32.14             72.27                         -1.0  \n",
      "6                   48.17             70.02                         -1.0  \n",
      "7                   39.89             56.91                         -1.0  \n",
      "8                   45.83             58.16                         -1.0  \n",
      "9                   47.03             65.61                         -1.0  \n",
      "10                  21.89             61.57                         -1.0  \n",
      "11                  74.38             81.39                         -1.0  \n",
      "12                  40.62             49.16                         -1.0  \n",
      "13                  61.74             82.55                         -1.0  \n",
      "14                  42.48             66.02                         -1.0  \n",
      "15                  30.17             75.21                         -1.0  \n",
      "16                  34.61             82.07                         -1.0  \n",
      "17                  41.41             92.04                         -1.0  \n",
      "18                  29.74             55.18                         -1.0  \n",
      "19                  27.98             68.20                         -1.0  \n",
      "20                  47.90             79.04                         -1.0  \n",
      "21                  37.07             76.77                         -1.0  \n",
      "22                  38.36             61.21                         -1.0  \n",
      "23                  69.87             65.11                         -1.0  \n",
      "24                  47.40             84.11                         -1.0  \n",
      "25                  48.67             63.64                         -1.0  \n",
      "26                  46.16             60.38                         -1.0  \n",
      "27                  47.27             53.70                         -1.0  \n",
      "28                  59.53             74.39                         -1.0  \n",
      "29                  58.60             66.26                         -1.0  \n",
      "...                   ...               ...                          ...  \n",
      "3245                52.07             60.40                          0.0  \n",
      "3246                54.52             60.40                          0.0  \n",
      "3247                46.75             62.60                          0.0  \n",
      "3248                53.52             60.10                          0.0  \n",
      "3249                87.57             53.00                          1.0  \n",
      "3250                20.67             70.00                          0.0  \n",
      "3251                89.55             55.20                          0.0  \n",
      "3252                79.96             73.40                          0.0  \n",
      "3253                42.80             86.50                         -2.0  \n",
      "3254                42.89             67.90                         -2.0  \n",
      "3255                32.24             64.30                         -2.0  \n",
      "3256                41.92             72.70                          2.0  \n",
      "3257                54.65             61.50                          0.0  \n",
      "3258                39.08             59.20                          0.0  \n",
      "3259                52.55             76.10                          0.0  \n",
      "3260                58.63             71.60                         -1.0  \n",
      "3261                70.24             47.80                          0.0  \n",
      "3262                80.14             85.80                          0.0  \n",
      "3263                29.61             66.30                         -1.0  \n",
      "3264                47.42             81.80                          0.0  \n",
      "3265                29.35             75.50                         -1.0  \n",
      "3266                10.51             70.10                         -1.0  \n",
      "3267                69.26             71.20                          1.0  \n",
      "3268                53.60             34.40                         -1.0  \n",
      "3269                35.54             65.00                          0.0  \n",
      "3270                44.13             72.00                          0.0  \n",
      "3271                88.58             56.20                         -1.0  \n",
      "3272                66.34             60.70                         -2.0  \n",
      "3273                80.14             88.40                          0.0  \n",
      "3274                44.72             72.80                          1.0  \n",
      "\n",
      "[3275 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# df_synth = pd.concat([synth_0, synth_minus1, synth_minus2, synth_1, synth_2], ignore_index=True)\n",
    "df_synth = pd.concat([synth_minus1, synth_minus2, synth_1, synth_2], ignore_index=True)\n",
    "df_synth['Discrete Thermal Comfort_TA'] = pd.to_numeric(df_synth['Discrete Thermal Comfort_TA'])\n",
    "print(df_synth['Discrete Thermal Comfort_TA'].value_counts())\n",
    "df_synth_train_balanced = pd.concat([df_synth, df_feature1_train], ignore_index=True)\n",
    "print(df_synth_train_balanced['Discrete Thermal Comfort_TA'].value_counts())\n",
    "print(df_synth_train_balanced)\n",
    "\n",
    "# approximate to 2 decimal points, as original dataset\n",
    "df_synth_train_balanced = df_synth_train_balanced.round(2)\n",
    "\n",
    "print(df_synth_train_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T07:05:12.139817Z",
     "start_time": "2019-08-05T07:05:12.137511Z"
    }
   },
   "source": [
    "#### Train: train set + synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:48:24.392182Z",
     "start_time": "2019-08-10T15:40:26.231180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Expected accuracy (f1 micro) based on Cross-Validation:  0.5786385657174122\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "Accuracy (f1 micro) on test set:  0.5725190839694656\n",
      "F1 micro on test set:  0.5725190839694656\n",
      "F1 macro on test set:  0.5035172440953011\n",
      "Confusion Matrix: \n",
      "[[103  26   3   2   4]\n",
      " [ 29 116  27   5   5]\n",
      " [ 32  30  34  56  11]\n",
      " [  0   5  14 114   4]\n",
      " [ 10   4   0  13   8]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -2.0       0.59      0.75      0.66       138\n",
      "        -1.0       0.64      0.64      0.64       182\n",
      "         0.0       0.44      0.21      0.28       163\n",
      "         1.0       0.60      0.83      0.70       137\n",
      "         2.0       0.25      0.23      0.24        35\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       655\n",
      "   macro avg       0.50      0.53      0.50       655\n",
      "weighted avg       0.55      0.57      0.55       655\n",
      "\n",
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Number of folds: 10\n",
      "Best parameters set found on development set:\n",
      "{'algorithm': 'brute', 'metric': 'seuclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='seuclidean',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "           weights='distance')\n",
      "Accuracy (f1 micro) on test set:  0.7633587786259542\n",
      "F1 micro on test set:  0.7633587786259542\n",
      "F1 macro on test set:  0.7461186028730733\n",
      "Confusion Matrix: \n",
      "[[116  11   5   1   5]\n",
      " [ 22 129  24   4   3]\n",
      " [  3  21 119  18   2]\n",
      " [  0   4  15 113   5]\n",
      " [  4   1   3   4  23]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -2.0       0.80      0.84      0.82       138\n",
      "        -1.0       0.78      0.71      0.74       182\n",
      "         0.0       0.72      0.73      0.72       163\n",
      "         1.0       0.81      0.82      0.82       137\n",
      "         2.0       0.61      0.66      0.63        35\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       655\n",
      "   macro avg       0.74      0.75      0.75       655\n",
      "weighted avg       0.76      0.76      0.76       655\n",
      "\n",
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Number of folds: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'C': 100, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=100, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy (f1 micro) on test set:  0.7374045801526717\n",
      "F1 micro on test set:  0.7374045801526719\n",
      "F1 macro on test set:  0.7261151840112642\n",
      "Confusion Matrix: \n",
      "[[118  12   0   1   7]\n",
      " [ 32 119  27   3   1]\n",
      " [  5  27 112  18   1]\n",
      " [  0   8  12 110   7]\n",
      " [  5   0   2   4  24]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -2.0       0.74      0.86      0.79       138\n",
      "        -1.0       0.72      0.65      0.68       182\n",
      "         0.0       0.73      0.69      0.71       163\n",
      "         1.0       0.81      0.80      0.81       137\n",
      "         2.0       0.60      0.69      0.64        35\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       655\n",
      "   macro avg       0.72      0.74      0.73       655\n",
      "weighted avg       0.74      0.74      0.74       655\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.24329159212880144\n",
      "F1 micro on test set:  0.24329159212880144\n",
      "F1 macro on test set:  0.10398231453395715\n",
      "Confusion Matrix: \n",
      "[[  0   0  11   0  34]\n",
      " [  0   0  51   0  98]\n",
      " [  0   0 132   0 202]\n",
      " [  0   0   2   0  25]\n",
      " [  0   0   0   0   4]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.00      0.00      0.00        45\n",
      "          -1       0.00      0.00      0.00       149\n",
      "           0       0.67      0.40      0.50       334\n",
      "           1       0.00      0.00      0.00        27\n",
      "           2       0.01      1.00      0.02         4\n",
      "\n",
      "   micro avg       0.24      0.24      0.24       559\n",
      "   macro avg       0.14      0.28      0.10       559\n",
      "weighted avg       0.40      0.24      0.30       559\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.3667262969588551\n",
      "F1 micro on test set:  0.3667262969588551\n",
      "F1 macro on test set:  0.24960025583626483\n",
      "Confusion Matrix: \n",
      "[[ 10  26   8   0   1]\n",
      " [ 27  88  26   8   0]\n",
      " [ 24  89  92 123   6]\n",
      " [  0   0  12  15   0]\n",
      " [  0   0   0   4   0]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.16      0.22      0.19        45\n",
      "          -1       0.43      0.59      0.50       149\n",
      "           0       0.67      0.28      0.39       334\n",
      "           1       0.10      0.56      0.17        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       559\n",
      "   macro avg       0.27      0.33      0.25       559\n",
      "weighted avg       0.53      0.37      0.39       559\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.5974955277280859\n",
      "F1 micro on test set:  0.5974955277280859\n",
      "F1 macro on test set:  0.1496080627099664\n",
      "Confusion Matrix: \n",
      "[[  0   0  45   0   0]\n",
      " [  0   0 149   0   0]\n",
      " [  0   0 334   0   0]\n",
      " [  0   0  27   0   0]\n",
      " [  0   0   4   0   0]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.00      0.00      0.00        45\n",
      "          -1       0.00      0.00      0.00       149\n",
      "           0       0.60      1.00      0.75       334\n",
      "           1       0.00      0.00      0.00        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.60      0.60      0.60       559\n",
      "   macro avg       0.12      0.20      0.15       559\n",
      "weighted avg       0.36      0.60      0.45       559\n",
      "\n",
      "original train\n",
      "0.5885509838998211\n",
      "0.5974955277280859\n",
      "0.5974955277280859\n",
      "\n",
      "\n",
      "more balanced train\n",
      "0.24329159212880144\n",
      "0.3667262969588551\n",
      "0.5974955277280859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# load best models NB, KNN, SVM, MLP\n",
    "acc_nb_synth_train_balanced, nb_optimal_synth_train_balanced = buildTrainNB(df_synth_train_balanced, test_size_percentage=test_size_percentage)\n",
    "acc_knn_synth_train_balanced, knn_optimal_synth_train_balanced = buildTrainKNN(df_synth_train_balanced, test_size_percentage=test_size_percentage)\n",
    "acc_svm_synth_train_balanced, svm_optimal_synth_train_balanced = buildTrainSVM(df_synth_train_balanced, test_size_percentage=test_size_percentage)\n",
    "\n",
    "# using the optimal model. re-train in whole train split and test in unseen test split\n",
    "acc_nb_synth_train_balanced_corr, _ = trainTest_tunedModel(df_feature1_test, nb_optimal_synth_train_balanced)\n",
    "acc_knn_synth_train_balanced_corr, _ = trainTest_tunedModel(df_feature1_test, knn_optimal_synth_train_balanced)\n",
    "acc_svm_synth_train_balanced_corr, _ = trainTest_tunedModel(df_feature1_test, svm_optimal_synth_train_balanced)\n",
    "\n",
    "print(\"original train\")\n",
    "print(acc_holistic_knn_1)\n",
    "print(acc_holistic_nb_1)\n",
    "print(acc_holistic_svm_1)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"more balanced train\")\n",
    "print(acc_nb_synth_train_balanced_corr)\n",
    "print(acc_knn_synth_train_balanced_corr)\n",
    "print(acc_svm_synth_train_balanced_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "original train\n",
    "0.5885509838998211\n",
    "0.5974955277280859\n",
    "0.5974955277280859\n",
    "\n",
    "more balanced train\n",
    "0.4883720930232558\n",
    "0.45080500894454384\n",
    "0.5974955277280859\n",
    "\n",
    "# twice synth\n",
    "0.49016100178890876\n",
    "0.42397137745974955\n",
    "0.5974955277280859\n",
    "\n",
    "# three times synth\n",
    "0.24329159212880144\n",
    "0.3667262969588551\n",
    "0.5974955277280859\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train: synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:51:53.781149Z",
     "start_time": "2019-08-10T15:48:27.195914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.0    818\n",
      "-1.0    606\n",
      "-2.0    516\n",
      " 1.0    513\n",
      " 2.0    132\n",
      "Name: Discrete Thermal Comfort_TA, dtype: int64\n",
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Expected accuracy (f1 micro) based on Cross-Validation:  0.6291441845311414\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "Accuracy (f1 micro) on test set:  0.6247582205029013\n",
      "F1 micro on test set:  0.6247582205029013\n",
      "F1 macro on test set:  0.6306682316947956\n",
      "Confusion Matrix: \n",
      "[[81 13  3  1  5]\n",
      " [19 65 33  3  1]\n",
      " [ 9 27 78 48  2]\n",
      " [ 0  3 16 83  1]\n",
      " [ 9  0  0  1 16]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -2.0       0.69      0.79      0.73       103\n",
      "        -1.0       0.60      0.54      0.57       121\n",
      "         0.0       0.60      0.48      0.53       164\n",
      "         1.0       0.61      0.81      0.69       103\n",
      "         2.0       0.64      0.62      0.63        26\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       517\n",
      "   macro avg       0.63      0.64      0.63       517\n",
      "weighted avg       0.62      0.62      0.62       517\n",
      "\n",
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Number of folds: 10\n",
      "Best parameters set found on development set:\n",
      "{'algorithm': 'brute', 'metric': 'seuclidean', 'n_neighbors': 12, 'weights': 'distance'}\n",
      "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='seuclidean',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=12, p=2,\n",
      "           weights='distance')\n",
      "Accuracy (f1 micro) on test set:  0.7040618955512572\n",
      "F1 micro on test set:  0.7040618955512572\n",
      "F1 macro on test set:  0.6879338512565247\n",
      "Confusion Matrix: \n",
      "[[ 99   2   1   1   0]\n",
      " [ 13  82  24   1   1]\n",
      " [ 11  27 100  26   0]\n",
      " [  1   1  29  72   0]\n",
      " [ 14   1   0   0  11]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -2.0       0.72      0.96      0.82       103\n",
      "        -1.0       0.73      0.68      0.70       121\n",
      "         0.0       0.65      0.61      0.63       164\n",
      "         1.0       0.72      0.70      0.71       103\n",
      "         2.0       0.92      0.42      0.58        26\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       517\n",
      "   macro avg       0.75      0.67      0.69       517\n",
      "weighted avg       0.71      0.70      0.70       517\n",
      "\n",
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Number of folds: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'C': 1000, 'class_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=100, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy (f1 micro) on test set:  0.7311411992263056\n",
      "F1 micro on test set:  0.7311411992263057\n",
      "F1 macro on test set:  0.7368413459377825\n",
      "Confusion Matrix: \n",
      "[[90  2  1  0 10]\n",
      " [12 93 12  3  1]\n",
      " [ 6 33 89 36  0]\n",
      " [ 2  1 15 84  1]\n",
      " [ 3  0  0  1 22]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -2.0       0.80      0.87      0.83       103\n",
      "        -1.0       0.72      0.77      0.74       121\n",
      "         0.0       0.76      0.54      0.63       164\n",
      "         1.0       0.68      0.82      0.74       103\n",
      "         2.0       0.65      0.85      0.73        26\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       517\n",
      "   macro avg       0.72      0.77      0.74       517\n",
      "weighted avg       0.74      0.73      0.73       517\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.23613595706618962\n",
      "F1 micro on test set:  0.23613595706618962\n",
      "F1 macro on test set:  0.08557536466774716\n",
      "Confusion Matrix: \n",
      "[[  0  45   0   0   0]\n",
      " [  0 132   0   0  17]\n",
      " [  0 265   0   0  69]\n",
      " [  0  22   0   0   5]\n",
      " [  0   4   0   0   0]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.00      0.00      0.00        45\n",
      "          -1       0.28      0.89      0.43       149\n",
      "           0       0.00      0.00      0.00       334\n",
      "           1       0.00      0.00      0.00        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.24      0.24      0.24       559\n",
      "   macro avg       0.06      0.18      0.09       559\n",
      "weighted avg       0.08      0.24      0.11       559\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.3953488372093023\n",
      "F1 micro on test set:  0.3953488372093023\n",
      "F1 macro on test set:  0.27787700242061303\n",
      "Confusion Matrix: \n",
      "[[ 15  18  11   0   1]\n",
      " [ 32  70  37   7   3]\n",
      " [ 26  82 116 110   0]\n",
      " [  1   0   6  20   0]\n",
      " [  0   0   0   4   0]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.20      0.33      0.25        45\n",
      "          -1       0.41      0.47      0.44       149\n",
      "           0       0.68      0.35      0.46       334\n",
      "           1       0.14      0.74      0.24        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.40      0.40      0.40       559\n",
      "   macro avg       0.29      0.38      0.28       559\n",
      "weighted avg       0.54      0.40      0.42       559\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.26654740608228983\n",
      "F1 micro on test set:  0.26654740608228983\n",
      "F1 macro on test set:  0.08418079096045197\n",
      "Confusion Matrix: \n",
      "[[  0  45   0   0   0]\n",
      " [  0 149   0   0   0]\n",
      " [  0 334   0   0   0]\n",
      " [  0  27   0   0   0]\n",
      " [  0   4   0   0   0]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.00      0.00      0.00        45\n",
      "          -1       0.27      1.00      0.42       149\n",
      "           0       0.00      0.00      0.00       334\n",
      "           1       0.00      0.00      0.00        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.27      0.27      0.27       559\n",
      "   macro avg       0.05      0.20      0.08       559\n",
      "weighted avg       0.07      0.27      0.11       559\n",
      "\n",
      "original train\n",
      "0.5885509838998211\n",
      "0.5974955277280859\n",
      "0.5974955277280859\n",
      "\n",
      "\n",
      "more balanced train\n",
      "0.23613595706618962\n",
      "0.3953488372093023\n",
      "0.26654740608228983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "df_synth = pd.concat([synth_0, synth_minus1, synth_minus2, synth_1, synth_2], ignore_index=True)\n",
    "print(df_synth['Discrete Thermal Comfort_TA'].value_counts())\n",
    "df_synth = df_synth.astype(float).round(2)\n",
    "\n",
    "# load best models NB, KNN, SVM, MLP\n",
    "acc_nb_synth, nb_optimal_synth = buildTrainNB(df_synth, test_size_percentage=test_size_percentage)\n",
    "acc_knn_synth, knn_optimal_synth = buildTrainKNN(df_synth, test_size_percentage=test_size_percentage)\n",
    "acc_svm_synth, svm_optimal_synth = buildTrainSVM(df_synth, test_size_percentage=test_size_percentage)\n",
    "\n",
    "# using the optimal model. re-train in whole train split and test in unseen test split\n",
    "acc_nb_synth, _ = trainTest_tunedModel(df_feature1_test, nb_optimal_synth)\n",
    "acc_knn_synth, _ = trainTest_tunedModel(df_feature1_test, knn_optimal_synth)\n",
    "acc_svm_synth, _ = trainTest_tunedModel(df_feature1_test, svm_optimal_synth)\n",
    "\n",
    "print(\"original train\")\n",
    "print(acc_holistic_knn_1)\n",
    "print(acc_holistic_nb_1)\n",
    "print(acc_holistic_svm_1)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"more balanced train\")\n",
    "print(acc_nb_synth)\n",
    "print(acc_knn_synth)\n",
    "print(acc_svm_synth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "original train\n",
    "0.5885509838998211\n",
    "0.5974955277280859\n",
    "0.5974955277280859\n",
    "\n",
    "# same size synth\n",
    "0.02146690518783542\n",
    "0.6100178890876565\n",
    "0.5974955277280859\n",
    "\n",
    "# twice synth\n",
    "0.6386404293381037\n",
    "0.47942754919499103\n",
    "0.5974955277280859\n",
    "\n",
    "# three times synth\n",
    "0.23613595706618962\n",
    "0.3953488372093023\n",
    "0.26654740608228983\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:00:04.543105Z",
     "start_time": "2019-08-10T13:57:41.432913Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# without corr\n",
    "EPOCHS = 1000 # 1000\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# testing\n",
    "gan_no_corr = GAN(data_dim=15,\n",
    "          n_hidden=150,\n",
    "          n_layers=3)\n",
    "\n",
    "gan_no_corr.train(df_feature1_train, EPOCHS=EPOCHS, use_corr_loss=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Samples needed: {}\".format(needed_0))\n",
    "synth_0_no = sample_needed_label(needed_0, 0, gan_no_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T16:03:33.242028Z",
     "start_time": "2019-08-10T15:51:56.789789Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples needed: 606\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "Samples needed: 516\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "Samples needed: 513\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "Samples needed: 132\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples needed: {}\".format(needed_minus1))\n",
    "synth_minus1 = sample_needed_label(needed_minus1, -1, gan_no_corr)\n",
    "\n",
    "print(\"Samples needed: {}\".format(needed_minus2))\n",
    "synth_minus2 = sample_needed_label(needed_minus2, -2, gan_no_corr)\n",
    "\n",
    "print(\"Samples needed: {}\".format(needed_1))\n",
    "synth_1 = sample_needed_label(needed_1, 1, gan_no_corr)\n",
    "\n",
    "print(\"Samples needed: {}\".format(needed_2))\n",
    "synth_2 = sample_needed_label(needed_2, 2, gan_no_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T16:03:35.850458Z",
     "start_time": "2019-08-10T16:03:35.821396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0    606\n",
      "-2.0    516\n",
      " 1.0    513\n",
      " 2.0    132\n",
      "Name: Discrete Thermal Comfort_TA, dtype: int64\n",
      "-1.0    909\n",
      " 0.0    818\n",
      "-2.0    688\n",
      " 1.0    684\n",
      " 2.0    176\n",
      "Name: Discrete Thermal Comfort_TA, dtype: int64\n",
      "      Temperature (Fahrenheit)  SkinTemperature  ClothingInsulation  \\\n",
      "0                    64.906837        82.567055            1.011260   \n",
      "1                    60.942684        87.683174            0.298949   \n",
      "2                    64.696869        78.387993            0.504262   \n",
      "3                    67.841232        89.023468            0.625165   \n",
      "4                    65.493042        73.105026            0.850259   \n",
      "5                    65.533546        79.727776            0.608644   \n",
      "6                    63.841270        91.600822            0.795266   \n",
      "7                    62.525509        87.305138            0.296868   \n",
      "8                    68.058464        73.444672            0.861508   \n",
      "9                    66.217041        84.631264            0.338741   \n",
      "10                   64.193924        88.356667            0.443827   \n",
      "11                   68.121025        92.202972            0.596802   \n",
      "12                   64.726982        88.238953            0.753944   \n",
      "13                   68.235817        82.203590            0.841411   \n",
      "14                   69.216583        85.011589            0.546947   \n",
      "15                   65.331459        84.331207            0.759800   \n",
      "16                   62.440102        83.520386            0.358257   \n",
      "17                   65.379707        82.325096            0.547163   \n",
      "18                   65.063416        84.609947            0.483719   \n",
      "19                   64.755356        84.875282            0.573287   \n",
      "20                   65.154770        84.241470            0.846875   \n",
      "21                   64.943405        70.603683            0.829004   \n",
      "22                   71.191132        87.971863            0.557873   \n",
      "23                   63.316723        89.750664            0.404833   \n",
      "24                   66.541313        82.115608            0.549742   \n",
      "25                   65.605087        84.073112            0.564466   \n",
      "26                   66.049576        90.747917            0.782412   \n",
      "27                   68.668182        90.070473            0.924321   \n",
      "28                   61.950016        84.447876            0.832983   \n",
      "29                   71.166229        69.620415            0.510968   \n",
      "...                        ...              ...                 ...   \n",
      "3245                 78.800003        90.635001            0.490000   \n",
      "3246                 64.400002        81.983839            0.650000   \n",
      "3247                 64.599998        80.755821            0.490000   \n",
      "3248                 65.599998        79.692337            0.650000   \n",
      "3249                 78.099998        88.514000            0.360000   \n",
      "3250                 76.099998        84.890000            1.070000   \n",
      "3251                 77.110001        89.821999            0.490000   \n",
      "3252                 77.540001        86.378000            0.490000   \n",
      "3253                 64.699997        85.382000            0.680000   \n",
      "3254                 64.000000        79.256000            0.580000   \n",
      "3255                 67.800003        82.544001            0.400000   \n",
      "3256                 80.099998        84.002001            0.580000   \n",
      "3257                 66.699997        98.631219            0.650000   \n",
      "3258                 70.199997        92.336001            0.710000   \n",
      "3259                 64.199997        86.696000            0.400000   \n",
      "3260                 64.400002        87.278001            0.570000   \n",
      "3261                 66.000000        80.510001            0.480000   \n",
      "3262                 77.349998        87.398001            0.320000   \n",
      "3263                 64.900002        81.139999            1.020000   \n",
      "3264                 74.900002        91.832000            0.400000   \n",
      "3265                 67.800003        89.408000            0.770000   \n",
      "3266                 65.599998        82.892001            0.680000   \n",
      "3267                 78.800003        86.323999            0.410000   \n",
      "3268                 66.000000        82.184099            0.710000   \n",
      "3269                 75.400002        86.120000            0.580000   \n",
      "3270                 71.199997        85.112947            0.490000   \n",
      "3271                 61.130001        84.128001            0.490000   \n",
      "3272                 63.799999        84.566000            0.660000   \n",
      "3273                 74.410004        86.306000            0.570000   \n",
      "3274                 77.599998        80.211576            0.490000   \n",
      "\n",
      "      Height(cm)  Shoulder Circumference(cm)  Weight(lbs)  Gender  \\\n",
      "0     181.600601                  129.361938   194.878510     1.0   \n",
      "1     187.013550                  125.508492   204.597565     1.0   \n",
      "2     174.813049                  110.364548   151.733063     1.0   \n",
      "3     166.578873                  102.159546   118.713264     0.0   \n",
      "4     178.767303                  111.449783   153.481400     1.0   \n",
      "5     168.707550                  107.467194   139.872070     0.0   \n",
      "6     162.702515                  107.820885   140.451477     0.0   \n",
      "7     167.831757                  108.473038   193.564560     0.0   \n",
      "8     175.480850                  110.832794   152.751495     1.0   \n",
      "9     169.401962                  103.476982   166.751053     0.0   \n",
      "10    171.891830                  106.482361   145.393066     0.0   \n",
      "11    163.833344                   97.973877   132.442612     0.0   \n",
      "12    160.270081                  106.691574   142.226669     0.0   \n",
      "13    172.767685                  111.712440   150.636322     1.0   \n",
      "14    160.854416                   97.449394   126.645485     0.0   \n",
      "15    163.053207                  107.300697   146.995010     0.0   \n",
      "16    169.531540                  101.672005   128.986771     0.0   \n",
      "17    159.080826                  105.426270   128.171356     0.0   \n",
      "18    165.170578                  108.321335   141.046524     0.0   \n",
      "19    174.543686                  107.821701   150.827698     1.0   \n",
      "20    162.508881                  101.254547   119.338806     0.0   \n",
      "21    185.689178                  119.746773   164.087662     1.0   \n",
      "22    160.336731                  101.346840   119.008293     0.0   \n",
      "23    166.862335                  107.243370   183.836517     0.0   \n",
      "24    175.797058                  113.480438   153.487503     1.0   \n",
      "25    163.446381                  107.506187   149.951691     0.0   \n",
      "26    165.674057                  101.022964   121.160194     0.0   \n",
      "27    159.977585                  105.616997   140.830353     0.0   \n",
      "28    170.725174                  117.131325   152.388687     1.0   \n",
      "29    186.131851                  108.756340   169.267883     1.0   \n",
      "...          ...                         ...          ...     ...   \n",
      "3245  162.000000                  101.000000   120.000000     0.0   \n",
      "3246  185.000000                  104.000000   115.000000     1.0   \n",
      "3247  169.000000                  108.500000   141.200000     0.0   \n",
      "3248  185.000000                  104.000000   115.000000     1.0   \n",
      "3249  170.200000                  104.200000   184.300000     0.0   \n",
      "3250  183.000000                  124.000000   182.000000     1.0   \n",
      "3251  155.400000                  102.400000   117.200000     0.0   \n",
      "3252  163.300000                  106.900000   166.200000     0.0   \n",
      "3253  173.000000                  101.000000   140.000000     0.0   \n",
      "3254  156.000000                   89.500000   111.000000     0.0   \n",
      "3255  157.500000                   92.000000    90.000000     0.0   \n",
      "3256  156.000000                   89.500000   111.000000     0.0   \n",
      "3257  185.000000                  104.000000   115.000000     1.0   \n",
      "3258  164.000000                  106.000000   159.400000     1.0   \n",
      "3259  189.000000                  130.000000   236.600000     1.0   \n",
      "3260  165.000000                  108.000000   146.000000     1.0   \n",
      "3261  177.000000                   94.500000   130.000000     0.0   \n",
      "3262  188.100000                  118.200000   185.700000     1.0   \n",
      "3263  164.000000                  101.600000   125.000000     0.0   \n",
      "3264  189.000000                  130.000000   236.600000     1.0   \n",
      "3265  158.000000                   98.000000   120.000000     0.0   \n",
      "3266  180.000000                  105.000000   143.300000     1.0   \n",
      "3267  176.700000                  106.200000   161.500000     1.0   \n",
      "3268  183.000000                  132.000000   170.000000     1.0   \n",
      "3269  175.000000                  112.000000   158.500000     1.0   \n",
      "3270  177.000000                  109.000000   145.000000     1.0   \n",
      "3271  155.400000                  102.400000   117.200000     0.0   \n",
      "3272  182.300000                  118.700000   167.600000     1.0   \n",
      "3273  188.100000                  118.200000   185.700000     1.0   \n",
      "3274  177.000000                  109.000000   145.000000     1.0   \n",
      "\n",
      "      Temperature_outside  Humidity_outside  Discrete Thermal Comfort_TA  \n",
      "0               29.183893         82.809235                         -1.0  \n",
      "1               64.779114         79.894806                         -1.0  \n",
      "2               44.892006         77.228699                         -1.0  \n",
      "3               53.446213         89.670647                         -1.0  \n",
      "4               34.493568         66.166016                         -1.0  \n",
      "5               30.453783         62.319946                         -1.0  \n",
      "6               48.989510         81.779091                         -1.0  \n",
      "7               73.641991         68.058357                         -1.0  \n",
      "8               41.196033         62.961411                         -1.0  \n",
      "9               80.149086         68.466270                         -1.0  \n",
      "10              40.960373         67.965103                         -1.0  \n",
      "11              52.179733         78.207108                         -1.0  \n",
      "12              59.765800         89.797646                         -1.0  \n",
      "13              19.963970         76.876846                         -1.0  \n",
      "14              10.756854         68.604713                         -1.0  \n",
      "15              47.504044         86.063828                         -1.0  \n",
      "16              46.383118         59.782227                         -1.0  \n",
      "17              42.167221         93.174828                         -1.0  \n",
      "18              44.193756        103.298210                         -1.0  \n",
      "19              56.132355         80.294685                         -1.0  \n",
      "20              20.495234         71.248001                         -1.0  \n",
      "21              45.628323         59.420013                         -1.0  \n",
      "22              46.225140        102.957558                         -1.0  \n",
      "23              80.411537         75.017822                         -1.0  \n",
      "24              46.168621         47.767853                         -1.0  \n",
      "25              63.615868         86.172394                         -1.0  \n",
      "26              38.569641         97.271683                         -1.0  \n",
      "27              40.142197         68.330765                         -1.0  \n",
      "28              41.999001         92.362373                         -1.0  \n",
      "29              20.509176         66.591614                         -1.0  \n",
      "...                   ...               ...                          ...  \n",
      "3245            52.070000         60.400002                          0.0  \n",
      "3246            54.520000         60.400002                          0.0  \n",
      "3247            46.750000         62.599998                          0.0  \n",
      "3248            53.520000         60.099998                          0.0  \n",
      "3249            87.570000         53.000000                          1.0  \n",
      "3250            20.670000         70.000000                          0.0  \n",
      "3251            89.550003         55.200001                          0.0  \n",
      "3252            79.959999         73.400002                          0.0  \n",
      "3253            42.799999         86.500000                         -2.0  \n",
      "3254            42.889999         67.900002                         -2.0  \n",
      "3255            32.240002         64.300003                         -2.0  \n",
      "3256            41.919998         72.699997                          2.0  \n",
      "3257            54.650002         61.500000                          0.0  \n",
      "3258            39.080002         59.200001                          0.0  \n",
      "3259            52.549999         76.099998                          0.0  \n",
      "3260            58.630001         71.599998                         -1.0  \n",
      "3261            70.239998         47.799999                          0.0  \n",
      "3262            80.139999         85.800003                          0.0  \n",
      "3263            29.610001         66.300003                         -1.0  \n",
      "3264            47.419998         81.800003                          0.0  \n",
      "3265            29.350000         75.500000                         -1.0  \n",
      "3266            10.510000         70.099998                         -1.0  \n",
      "3267            69.260002         71.199997                          1.0  \n",
      "3268            53.599998         34.400002                         -1.0  \n",
      "3269            35.540001         65.000000                          0.0  \n",
      "3270            44.130001         72.000000                          0.0  \n",
      "3271            88.580002         56.200001                         -1.0  \n",
      "3272            66.339996         60.700001                         -2.0  \n",
      "3273            80.139999         88.400002                          0.0  \n",
      "3274            44.720001         72.800003                          1.0  \n",
      "\n",
      "[3275 rows x 10 columns]\n",
      "      Temperature (Fahrenheit)  SkinTemperature  ClothingInsulation  \\\n",
      "0                        64.91            82.57                1.01   \n",
      "1                        60.94            87.68                0.30   \n",
      "2                        64.70            78.39                0.50   \n",
      "3                        67.84            89.02                0.63   \n",
      "4                        65.49            73.11                0.85   \n",
      "5                        65.53            79.73                0.61   \n",
      "6                        63.84            91.60                0.80   \n",
      "7                        62.53            87.31                0.30   \n",
      "8                        68.06            73.44                0.86   \n",
      "9                        66.22            84.63                0.34   \n",
      "10                       64.19            88.36                0.44   \n",
      "11                       68.12            92.20                0.60   \n",
      "12                       64.73            88.24                0.75   \n",
      "13                       68.24            82.20                0.84   \n",
      "14                       69.22            85.01                0.55   \n",
      "15                       65.33            84.33                0.76   \n",
      "16                       62.44            83.52                0.36   \n",
      "17                       65.38            82.33                0.55   \n",
      "18                       65.06            84.61                0.48   \n",
      "19                       64.76            84.88                0.57   \n",
      "20                       65.15            84.24                0.85   \n",
      "21                       64.94            70.60                0.83   \n",
      "22                       71.19            87.97                0.56   \n",
      "23                       63.32            89.75                0.40   \n",
      "24                       66.54            82.12                0.55   \n",
      "25                       65.61            84.07                0.56   \n",
      "26                       66.05            90.75                0.78   \n",
      "27                       68.67            90.07                0.92   \n",
      "28                       61.95            84.45                0.83   \n",
      "29                       71.17            69.62                0.51   \n",
      "...                        ...              ...                 ...   \n",
      "3245                     78.80            90.64                0.49   \n",
      "3246                     64.40            81.98                0.65   \n",
      "3247                     64.60            80.76                0.49   \n",
      "3248                     65.60            79.69                0.65   \n",
      "3249                     78.10            88.51                0.36   \n",
      "3250                     76.10            84.89                1.07   \n",
      "3251                     77.11            89.82                0.49   \n",
      "3252                     77.54            86.38                0.49   \n",
      "3253                     64.70            85.38                0.68   \n",
      "3254                     64.00            79.26                0.58   \n",
      "3255                     67.80            82.54                0.40   \n",
      "3256                     80.10            84.00                0.58   \n",
      "3257                     66.70            98.63                0.65   \n",
      "3258                     70.20            92.34                0.71   \n",
      "3259                     64.20            86.70                0.40   \n",
      "3260                     64.40            87.28                0.57   \n",
      "3261                     66.00            80.51                0.48   \n",
      "3262                     77.35            87.40                0.32   \n",
      "3263                     64.90            81.14                1.02   \n",
      "3264                     74.90            91.83                0.40   \n",
      "3265                     67.80            89.41                0.77   \n",
      "3266                     65.60            82.89                0.68   \n",
      "3267                     78.80            86.32                0.41   \n",
      "3268                     66.00            82.18                0.71   \n",
      "3269                     75.40            86.12                0.58   \n",
      "3270                     71.20            85.11                0.49   \n",
      "3271                     61.13            84.13                0.49   \n",
      "3272                     63.80            84.57                0.66   \n",
      "3273                     74.41            86.31                0.57   \n",
      "3274                     77.60            80.21                0.49   \n",
      "\n",
      "      Height(cm)  Shoulder Circumference(cm)  Weight(lbs)  Gender  \\\n",
      "0         181.60                      129.36       194.88     1.0   \n",
      "1         187.01                      125.51       204.60     1.0   \n",
      "2         174.81                      110.36       151.73     1.0   \n",
      "3         166.58                      102.16       118.71     0.0   \n",
      "4         178.77                      111.45       153.48     1.0   \n",
      "5         168.71                      107.47       139.87     0.0   \n",
      "6         162.70                      107.82       140.45     0.0   \n",
      "7         167.83                      108.47       193.56     0.0   \n",
      "8         175.48                      110.83       152.75     1.0   \n",
      "9         169.40                      103.48       166.75     0.0   \n",
      "10        171.89                      106.48       145.39     0.0   \n",
      "11        163.83                       97.97       132.44     0.0   \n",
      "12        160.27                      106.69       142.23     0.0   \n",
      "13        172.77                      111.71       150.64     1.0   \n",
      "14        160.85                       97.45       126.65     0.0   \n",
      "15        163.05                      107.30       147.00     0.0   \n",
      "16        169.53                      101.67       128.99     0.0   \n",
      "17        159.08                      105.43       128.17     0.0   \n",
      "18        165.17                      108.32       141.05     0.0   \n",
      "19        174.54                      107.82       150.83     1.0   \n",
      "20        162.51                      101.25       119.34     0.0   \n",
      "21        185.69                      119.75       164.09     1.0   \n",
      "22        160.34                      101.35       119.01     0.0   \n",
      "23        166.86                      107.24       183.84     0.0   \n",
      "24        175.80                      113.48       153.49     1.0   \n",
      "25        163.45                      107.51       149.95     0.0   \n",
      "26        165.67                      101.02       121.16     0.0   \n",
      "27        159.98                      105.62       140.83     0.0   \n",
      "28        170.73                      117.13       152.39     1.0   \n",
      "29        186.13                      108.76       169.27     1.0   \n",
      "...          ...                         ...          ...     ...   \n",
      "3245      162.00                      101.00       120.00     0.0   \n",
      "3246      185.00                      104.00       115.00     1.0   \n",
      "3247      169.00                      108.50       141.20     0.0   \n",
      "3248      185.00                      104.00       115.00     1.0   \n",
      "3249      170.20                      104.20       184.30     0.0   \n",
      "3250      183.00                      124.00       182.00     1.0   \n",
      "3251      155.40                      102.40       117.20     0.0   \n",
      "3252      163.30                      106.90       166.20     0.0   \n",
      "3253      173.00                      101.00       140.00     0.0   \n",
      "3254      156.00                       89.50       111.00     0.0   \n",
      "3255      157.50                       92.00        90.00     0.0   \n",
      "3256      156.00                       89.50       111.00     0.0   \n",
      "3257      185.00                      104.00       115.00     1.0   \n",
      "3258      164.00                      106.00       159.40     1.0   \n",
      "3259      189.00                      130.00       236.60     1.0   \n",
      "3260      165.00                      108.00       146.00     1.0   \n",
      "3261      177.00                       94.50       130.00     0.0   \n",
      "3262      188.10                      118.20       185.70     1.0   \n",
      "3263      164.00                      101.60       125.00     0.0   \n",
      "3264      189.00                      130.00       236.60     1.0   \n",
      "3265      158.00                       98.00       120.00     0.0   \n",
      "3266      180.00                      105.00       143.30     1.0   \n",
      "3267      176.70                      106.20       161.50     1.0   \n",
      "3268      183.00                      132.00       170.00     1.0   \n",
      "3269      175.00                      112.00       158.50     1.0   \n",
      "3270      177.00                      109.00       145.00     1.0   \n",
      "3271      155.40                      102.40       117.20     0.0   \n",
      "3272      182.30                      118.70       167.60     1.0   \n",
      "3273      188.10                      118.20       185.70     1.0   \n",
      "3274      177.00                      109.00       145.00     1.0   \n",
      "\n",
      "      Temperature_outside  Humidity_outside  Discrete Thermal Comfort_TA  \n",
      "0                   29.18             82.81                         -1.0  \n",
      "1                   64.78             79.89                         -1.0  \n",
      "2                   44.89             77.23                         -1.0  \n",
      "3                   53.45             89.67                         -1.0  \n",
      "4                   34.49             66.17                         -1.0  \n",
      "5                   30.45             62.32                         -1.0  \n",
      "6                   48.99             81.78                         -1.0  \n",
      "7                   73.64             68.06                         -1.0  \n",
      "8                   41.20             62.96                         -1.0  \n",
      "9                   80.15             68.47                         -1.0  \n",
      "10                  40.96             67.97                         -1.0  \n",
      "11                  52.18             78.21                         -1.0  \n",
      "12                  59.77             89.80                         -1.0  \n",
      "13                  19.96             76.88                         -1.0  \n",
      "14                  10.76             68.60                         -1.0  \n",
      "15                  47.50             86.06                         -1.0  \n",
      "16                  46.38             59.78                         -1.0  \n",
      "17                  42.17             93.17                         -1.0  \n",
      "18                  44.19            103.30                         -1.0  \n",
      "19                  56.13             80.29                         -1.0  \n",
      "20                  20.50             71.25                         -1.0  \n",
      "21                  45.63             59.42                         -1.0  \n",
      "22                  46.23            102.96                         -1.0  \n",
      "23                  80.41             75.02                         -1.0  \n",
      "24                  46.17             47.77                         -1.0  \n",
      "25                  63.62             86.17                         -1.0  \n",
      "26                  38.57             97.27                         -1.0  \n",
      "27                  40.14             68.33                         -1.0  \n",
      "28                  42.00             92.36                         -1.0  \n",
      "29                  20.51             66.59                         -1.0  \n",
      "...                   ...               ...                          ...  \n",
      "3245                52.07             60.40                          0.0  \n",
      "3246                54.52             60.40                          0.0  \n",
      "3247                46.75             62.60                          0.0  \n",
      "3248                53.52             60.10                          0.0  \n",
      "3249                87.57             53.00                          1.0  \n",
      "3250                20.67             70.00                          0.0  \n",
      "3251                89.55             55.20                          0.0  \n",
      "3252                79.96             73.40                          0.0  \n",
      "3253                42.80             86.50                         -2.0  \n",
      "3254                42.89             67.90                         -2.0  \n",
      "3255                32.24             64.30                         -2.0  \n",
      "3256                41.92             72.70                          2.0  \n",
      "3257                54.65             61.50                          0.0  \n",
      "3258                39.08             59.20                          0.0  \n",
      "3259                52.55             76.10                          0.0  \n",
      "3260                58.63             71.60                         -1.0  \n",
      "3261                70.24             47.80                          0.0  \n",
      "3262                80.14             85.80                          0.0  \n",
      "3263                29.61             66.30                         -1.0  \n",
      "3264                47.42             81.80                          0.0  \n",
      "3265                29.35             75.50                         -1.0  \n",
      "3266                10.51             70.10                         -1.0  \n",
      "3267                69.26             71.20                          1.0  \n",
      "3268                53.60             34.40                         -1.0  \n",
      "3269                35.54             65.00                          0.0  \n",
      "3270                44.13             72.00                          0.0  \n",
      "3271                88.58             56.20                         -1.0  \n",
      "3272                66.34             60.70                         -2.0  \n",
      "3273                80.14             88.40                          0.0  \n",
      "3274                44.72             72.80                          1.0  \n",
      "\n",
      "[3275 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# df_synth = pd.concat([synth_0, synth_minus1, synth_minus2, synth_1, synth_2], ignore_index=True)\n",
    "df_synth = pd.concat([synth_minus1, synth_minus2, synth_1, synth_2], ignore_index=True)\n",
    "df_synth['Discrete Thermal Comfort_TA'] = pd.to_numeric(df_synth['Discrete Thermal Comfort_TA'])\n",
    "print(df_synth['Discrete Thermal Comfort_TA'].value_counts())\n",
    "df_synth_train_balanced = pd.concat([df_synth, df_feature1_train], ignore_index=True)\n",
    "print(df_synth_train_balanced['Discrete Thermal Comfort_TA'].value_counts())\n",
    "print(df_synth_train_balanced)\n",
    "\n",
    "# approximate to 2 decimal points, as original dataset\n",
    "df_synth_train_balanced = df_synth_train_balanced.round(2)\n",
    "\n",
    "print(df_synth_train_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train: train + synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T16:10:37.463144Z",
     "start_time": "2019-08-10T16:03:38.404260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Expected accuracy (f1 micro) based on Cross-Validation:  0.5389308491409905\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "Accuracy (f1 micro) on test set:  0.5145038167938931\n",
      "F1 micro on test set:  0.5145038167938931\n",
      "F1 macro on test set:  0.4634815100561875\n",
      "Confusion Matrix: \n",
      "[[ 48  42  20   2  26]\n",
      " [ 21 135  19   5   2]\n",
      " [ 30  47  30  46  10]\n",
      " [  6   0  14 108   9]\n",
      " [  6   1   1  11  16]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -2.0       0.43      0.35      0.39       138\n",
      "        -1.0       0.60      0.74      0.66       182\n",
      "         0.0       0.36      0.18      0.24       163\n",
      "         1.0       0.63      0.79      0.70       137\n",
      "         2.0       0.25      0.46      0.33        35\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       655\n",
      "   macro avg       0.45      0.50      0.46       655\n",
      "weighted avg       0.49      0.51      0.49       655\n",
      "\n",
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Number of folds: 10\n",
      "Best parameters set found on development set:\n",
      "{'algorithm': 'brute', 'metric': 'seuclidean', 'n_neighbors': 9, 'weights': 'distance'}\n",
      "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='seuclidean',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=9, p=2,\n",
      "           weights='distance')\n",
      "Accuracy (f1 micro) on test set:  0.7236641221374046\n",
      "F1 micro on test set:  0.7236641221374046\n",
      "F1 macro on test set:  0.6890635529345206\n",
      "Confusion Matrix: \n",
      "[[100  19  10   6   3]\n",
      " [ 17 131  30   3   1]\n",
      " [  3  25 118  16   1]\n",
      " [  6   2  16 111   2]\n",
      " [  9   1   4   7  14]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -2.0       0.74      0.72      0.73       138\n",
      "        -1.0       0.74      0.72      0.73       182\n",
      "         0.0       0.66      0.72      0.69       163\n",
      "         1.0       0.78      0.81      0.79       137\n",
      "         2.0       0.67      0.40      0.50        35\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       655\n",
      "   macro avg       0.72      0.68      0.69       655\n",
      "weighted avg       0.72      0.72      0.72       655\n",
      "\n",
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Number of folds: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'C': 100, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=100, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy (f1 micro) on test set:  0.7526717557251908\n",
      "F1 micro on test set:  0.7526717557251908\n",
      "F1 macro on test set:  0.7224225606001078\n",
      "Confusion Matrix: \n",
      "[[117  10   3   2   6]\n",
      " [ 16 132  31   2   1]\n",
      " [  4  21 113  23   2]\n",
      " [  7   1   8 111  10]\n",
      " [  3   2   4   6  20]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -2.0       0.80      0.85      0.82       138\n",
      "        -1.0       0.80      0.73      0.76       182\n",
      "         0.0       0.71      0.69      0.70       163\n",
      "         1.0       0.77      0.81      0.79       137\n",
      "         2.0       0.51      0.57      0.54        35\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       655\n",
      "   macro avg       0.72      0.73      0.72       655\n",
      "weighted avg       0.75      0.75      0.75       655\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.08050089445438283\n",
      "F1 micro on test set:  0.08050089445438283\n",
      "F1 macro on test set:  0.02980132450331126\n",
      "Confusion Matrix: \n",
      "[[ 45   0   0   0   0]\n",
      " [149   0   0   0   0]\n",
      " [334   0   0   0   0]\n",
      " [ 27   0   0   0   0]\n",
      " [  4   0   0   0   0]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.08      1.00      0.15        45\n",
      "          -1       0.00      0.00      0.00       149\n",
      "           0       0.00      0.00      0.00       334\n",
      "           1       0.00      0.00      0.00        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.08      0.08      0.08       559\n",
      "   macro avg       0.02      0.20      0.03       559\n",
      "weighted avg       0.01      0.08      0.01       559\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.4597495527728086\n",
      "F1 micro on test set:  0.4597495527728086\n",
      "F1 macro on test set:  0.32921584518888924\n",
      "Confusion Matrix: \n",
      "[[ 18  23   4   0   0]\n",
      " [ 30 100  16   3   0]\n",
      " [ 18  90 118  98  10]\n",
      " [  0   0   5  21   1]\n",
      " [  0   0   0   4   0]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.27      0.40      0.32        45\n",
      "          -1       0.47      0.67      0.55       149\n",
      "           0       0.83      0.35      0.49       334\n",
      "           1       0.17      0.78      0.27        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.46      0.46      0.46       559\n",
      "   macro avg       0.35      0.44      0.33       559\n",
      "weighted avg       0.65      0.46      0.48       559\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.08050089445438283\n",
      "F1 micro on test set:  0.08050089445438283\n",
      "F1 macro on test set:  0.02980132450331126\n",
      "Confusion Matrix: \n",
      "[[ 45   0   0   0   0]\n",
      " [149   0   0   0   0]\n",
      " [334   0   0   0   0]\n",
      " [ 27   0   0   0   0]\n",
      " [  4   0   0   0   0]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.08      1.00      0.15        45\n",
      "          -1       0.00      0.00      0.00       149\n",
      "           0       0.00      0.00      0.00       334\n",
      "           1       0.00      0.00      0.00        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.08      0.08      0.08       559\n",
      "   macro avg       0.02      0.20      0.03       559\n",
      "weighted avg       0.01      0.08      0.01       559\n",
      "\n",
      "original train\n",
      "0.5885509838998211\n",
      "0.5974955277280859\n",
      "0.5974955277280859\n",
      "\n",
      "\n",
      "more balanced train\n",
      "0.08050089445438283\n",
      "0.4597495527728086\n",
      "0.08050089445438283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# load best models NB, KNN, SVM, MLP\n",
    "acc_nb_synth_train_balanced, nb_optimal_synth_train_balanced = buildTrainNB(df_synth_train_balanced, test_size_percentage=test_size_percentage)\n",
    "acc_knn_synth_train_balanced, knn_optimal_synth_train_balanced = buildTrainKNN(df_synth_train_balanced, test_size_percentage=test_size_percentage)\n",
    "acc_svm_synth_train_balanced, svm_optimal_synth_train_balanced = buildTrainSVM(df_synth_train_balanced, test_size_percentage=test_size_percentage)\n",
    "\n",
    "# using the optimal model. re-train in whole train split and test in unseen test split\n",
    "acc_nb_synth_train_balanced_corr, _ = trainTest_tunedModel(df_feature1_test, nb_optimal_synth_train_balanced)\n",
    "acc_knn_synth_train_balanced_corr, _ = trainTest_tunedModel(df_feature1_test, knn_optimal_synth_train_balanced)\n",
    "acc_svm_synth_train_balanced_corr, _ = trainTest_tunedModel(df_feature1_test, svm_optimal_synth_train_balanced)\n",
    "\n",
    "print(\"original train\")\n",
    "print(acc_holistic_knn_1)\n",
    "print(acc_holistic_nb_1)\n",
    "print(acc_holistic_svm_1)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"more balanced train\")\n",
    "print(acc_nb_synth_train_balanced_corr)\n",
    "print(acc_knn_synth_train_balanced_corr)\n",
    "print(acc_svm_synth_train_balanced_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "original train\n",
    "0.5885509838998211\n",
    "0.5974955277280859\n",
    "0.5974955277280859\n",
    "\n",
    "# same size synth\n",
    "0.08050089445438283\n",
    "0.4919499105545617\n",
    "0.5974955277280859\n",
    "\n",
    "# twice synth\n",
    "0.08050089445438283\n",
    "0.4186046511627907\n",
    "0.08050089445438283\n",
    "\n",
    "# three times synth\n",
    "0.08050089445438283\n",
    "0.4597495527728086\n",
    "0.08050089445438283\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train: synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T16:14:58.531069Z",
     "start_time": "2019-08-10T16:10:40.008312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.0    818\n",
      "-1.0    606\n",
      "-2.0    516\n",
      " 1.0    513\n",
      " 2.0    132\n",
      "Name: Discrete Thermal Comfort_TA, dtype: int64\n",
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Expected accuracy (f1 micro) based on Cross-Validation:  0.5836063480056695\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "Accuracy (f1 micro) on test set:  0.620889748549323\n",
      "F1 micro on test set:  0.620889748549323\n",
      "F1 macro on test set:  0.6017934396812754\n",
      "Confusion Matrix: \n",
      "[[50 13 15  1 24]\n",
      " [ 8 97 13  0  3]\n",
      " [11 40 82 22  9]\n",
      " [ 4  0 22 72  5]\n",
      " [ 1  5  0  0 20]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -2.0       0.68      0.49      0.56       103\n",
      "        -1.0       0.63      0.80      0.70       121\n",
      "         0.0       0.62      0.50      0.55       164\n",
      "         1.0       0.76      0.70      0.73       103\n",
      "         2.0       0.33      0.77      0.46        26\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       517\n",
      "   macro avg       0.60      0.65      0.60       517\n",
      "weighted avg       0.65      0.62      0.62       517\n",
      "\n",
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Number of folds: 10\n",
      "Best parameters set found on development set:\n",
      "{'algorithm': 'brute', 'metric': 'seuclidean', 'n_neighbors': 10, 'weights': 'distance'}\n",
      "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='seuclidean',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "           weights='distance')\n",
      "Accuracy (f1 micro) on test set:  0.7601547388781431\n",
      "F1 micro on test set:  0.760154738878143\n",
      "F1 macro on test set:  0.748712125493752\n",
      "Confusion Matrix: \n",
      "[[ 89   5   4   4   1]\n",
      " [  4  98  17   1   1]\n",
      " [  8  32 112  11   1]\n",
      " [  4   1  18  80   0]\n",
      " [  7   3   2   0  14]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -2.0       0.79      0.86      0.83       103\n",
      "        -1.0       0.71      0.81      0.75       121\n",
      "         0.0       0.73      0.68      0.71       164\n",
      "         1.0       0.83      0.78      0.80       103\n",
      "         2.0       0.82      0.54      0.65        26\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       517\n",
      "   macro avg       0.78      0.73      0.75       517\n",
      "weighted avg       0.76      0.76      0.76       517\n",
      "\n",
      "Features: ['Temperature (Fahrenheit)' 'SkinTemperature' 'ClothingInsulation'\n",
      " 'Height(cm)' 'Shoulder Circumference(cm)' 'Weight(lbs)' 'Gender'\n",
      " 'Temperature_outside' 'Humidity_outside']\n",
      "Number of folds: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'C': 100, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=100, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy (f1 micro) on test set:  0.7717601547388782\n",
      "F1 micro on test set:  0.7717601547388783\n",
      "F1 macro on test set:  0.7584104547822795\n",
      "Confusion Matrix: \n",
      "[[ 89   1   5   6   2]\n",
      " [  9  94  14   2   2]\n",
      " [  8  21 120  13   2]\n",
      " [  1   3  19  80   0]\n",
      " [  6   1   2   1  16]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -2.0       0.79      0.86      0.82       103\n",
      "        -1.0       0.78      0.78      0.78       121\n",
      "         0.0       0.75      0.73      0.74       164\n",
      "         1.0       0.78      0.78      0.78       103\n",
      "         2.0       0.73      0.62      0.67        26\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       517\n",
      "   macro avg       0.77      0.75      0.76       517\n",
      "weighted avg       0.77      0.77      0.77       517\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.08050089445438283\n",
      "F1 micro on test set:  0.08050089445438283\n",
      "F1 macro on test set:  0.02980132450331126\n",
      "Confusion Matrix: \n",
      "[[ 45   0   0   0   0]\n",
      " [149   0   0   0   0]\n",
      " [334   0   0   0   0]\n",
      " [ 27   0   0   0   0]\n",
      " [  4   0   0   0   0]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.08      1.00      0.15        45\n",
      "          -1       0.00      0.00      0.00       149\n",
      "           0       0.00      0.00      0.00       334\n",
      "           1       0.00      0.00      0.00        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.08      0.08      0.08       559\n",
      "   macro avg       0.02      0.20      0.03       559\n",
      "weighted avg       0.01      0.08      0.01       559\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.5080500894454383\n",
      "F1 micro on test set:  0.5080500894454383\n",
      "F1 macro on test set:  0.35026129426129426\n",
      "Confusion Matrix: \n",
      "[[ 13  21  10   0   1]\n",
      " [ 17  99  26   3   4]\n",
      " [ 15  82 151  75  11]\n",
      " [  0   0   4  21   2]\n",
      " [  0   0   0   4   0]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.29      0.29      0.29        45\n",
      "          -1       0.49      0.66      0.56       149\n",
      "           0       0.79      0.45      0.58       334\n",
      "           1       0.20      0.78      0.32        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       559\n",
      "   macro avg       0.35      0.44      0.35       559\n",
      "weighted avg       0.64      0.51      0.53       559\n",
      "\n",
      "Accuracy (f1 micro) on test set:  0.08050089445438283\n",
      "F1 micro on test set:  0.08050089445438283\n",
      "F1 macro on test set:  0.02980132450331126\n",
      "Confusion Matrix: \n",
      "[[ 45   0   0   0   0]\n",
      " [149   0   0   0   0]\n",
      " [334   0   0   0   0]\n",
      " [ 27   0   0   0   0]\n",
      " [  4   0   0   0   0]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       0.08      1.00      0.15        45\n",
      "          -1       0.00      0.00      0.00       149\n",
      "           0       0.00      0.00      0.00       334\n",
      "           1       0.00      0.00      0.00        27\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.08      0.08      0.08       559\n",
      "   macro avg       0.02      0.20      0.03       559\n",
      "weighted avg       0.01      0.08      0.01       559\n",
      "\n",
      "original train\n",
      "0.5885509838998211\n",
      "0.5974955277280859\n",
      "0.5974955277280859\n",
      "\n",
      "\n",
      "more balanced train\n",
      "0.08050089445438283\n",
      "0.5080500894454383\n",
      "0.08050089445438283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matias/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "df_synth = pd.concat([synth_0, synth_minus1, synth_minus2, synth_1, synth_2], ignore_index=True)\n",
    "print(df_synth['Discrete Thermal Comfort_TA'].value_counts())\n",
    "df_synth = df_synth.astype(float).round(2)\n",
    "\n",
    "# load best models NB, KNN, SVM, MLP\n",
    "acc_nb_synth, nb_optimal_synth = buildTrainNB(df_synth, test_size_percentage=test_size_percentage)\n",
    "acc_knn_synth, knn_optimal_synth = buildTrainKNN(df_synth, test_size_percentage=test_size_percentage)\n",
    "acc_svm_synth, svm_optimal_synth = buildTrainSVM(df_synth, test_size_percentage=test_size_percentage)\n",
    "\n",
    "# using the optimal model. re-train in whole train split and test in unseen test split\n",
    "acc_nb_synth, _ = trainTest_tunedModel(df_feature1_test, nb_optimal_synth)\n",
    "acc_knn_synth, _ = trainTest_tunedModel(df_feature1_test, knn_optimal_synth)\n",
    "acc_svm_synth, _ = trainTest_tunedModel(df_feature1_test, svm_optimal_synth)\n",
    "\n",
    "print(\"original train\")\n",
    "print(acc_holistic_knn_1)\n",
    "print(acc_holistic_nb_1)\n",
    "print(acc_holistic_svm_1)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"more balanced train\")\n",
    "print(acc_nb_synth)\n",
    "print(acc_knn_synth)\n",
    "print(acc_svm_synth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "original train\n",
    "0.5885509838998211\n",
    "0.5974955277280859\n",
    "0.5974955277280859\n",
    "\n",
    "# same size synth\n",
    "0.08050089445438283\n",
    "0.631484794275492\n",
    "0.5974955277280859\n",
    "\n",
    "# twice synth\n",
    "0.08050089445438283\n",
    "0.5724508050089445\n",
    "0.5974955277280859\n",
    "\n",
    "# three times synth\n",
    "0.08050089445438283\n",
    "0.5080500894454383\n",
    "0.08050089445438283\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "395.85px",
    "left": "-1536px",
    "right": "20px",
    "top": "365px",
    "width": "576px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
